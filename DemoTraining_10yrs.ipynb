{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeXGLNC4alVU",
        "outputId": "4cf311ca-cc3c-4a6c-aa15-96c3ad7be65f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/site-packages (4.1.1.post0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: wrds in /usr/local/lib/python3.10/site-packages (3.1.6)\n",
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.10/site-packages (from wrds) (2.9.9)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from wrds) (2.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from wrds) (1.26.2)\n",
            "Requirement already satisfied: sqlalchemy<2 in /usr/local/lib/python3.10/site-packages (from wrds) (1.4.50)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from wrds) (1.11.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/site-packages (from sqlalchemy<2->wrds) (3.0.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->wrds) (2023.3.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->wrds) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->wrds) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->wrds) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pyportfolioopt in /usr/local/lib/python3.10/site-packages (1.5.5)\n",
            "Requirement already satisfied: cvxpy<2.0.0,>=1.1.19 in /usr/local/lib/python3.10/site-packages (from pyportfolioopt) (1.4.1)\n",
            "Requirement already satisfied: scipy<2.0,>=1.3 in /usr/local/lib/python3.10/site-packages (from pyportfolioopt) (1.11.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/site-packages (from pyportfolioopt) (2.1.3)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.4 in /usr/local/lib/python3.10/site-packages (from pyportfolioopt) (1.26.2)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (0.6.0)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (2.0.12)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (0.6.3)\n",
            "Requirement already satisfied: scs>=3.0 in /usr/local/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (3.2.4.post1)\n",
            "Requirement already satisfied: pybind11 in /usr/local/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (2.11.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas>=0.19->pyportfolioopt) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas>=0.19->pyportfolioopt) (2023.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas>=0.19->pyportfolioopt) (2.8.2)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/site-packages (from osqp>=0.6.2->cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (0.1.7.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pyportfolioopt) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m‚ú®üç∞‚ú® Everything looks OK!\n",
            "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
            "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /tmp/pip-req-build-ou6j21te\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /tmp/pip-req-build-ou6j21te\n",
            "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 1410c340ded1a2fb49b16caf972e55b3d1d6efbb\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-8dr1mnug/elegantrl_26147765953f481188d519ee091d6248\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-8dr1mnug/elegantrl_26147765953f481188d519ee091d6248\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit b4b9d662b9f9cb7cc368ac2b1036b5119eb20be4\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wrds<4,>=3 in /usr/local/lib/python3.10/site-packages (from finrl==0.3.6) (3.1.6)\n",
            "Requirement already satisfied: scikit-learn<2,>=1 in /usr/local/lib/python3.10/site-packages (from finrl==0.3.6) (1.3.2)\n",
            "Requirement already satisfied: stable-baselines3[extra]>=2.0.0a5 in /usr/local/lib/python3.10/site-packages (from finrl==0.3.6) (2.2.1)\n",
            "Requirement already satisfied: ccxt<4,>=3 in /usr/local/lib/python3.10/site-packages (from finrl==0.3.6) (3.1.60)\n",
            "Requirement already satisfied: alpaca-trade-api<4,>=3 in /usr/local/lib/python3.10/site-packages (from finrl==0.3.6) (3.0.2)\n",
            "Requirement already satisfied: pyportfolioopt<2,>=1 in /usr/local/lib/python3.10/site-packages (from finrl==0.3.6) (1.5.5)\n",
            "Requirement already satisfied: stockstats<0.6,>=0.5 in /usr/local/lib/python3.10/site-packages (from finrl==0.3.6) (0.5.4)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /usr/local/lib/python3.10/site-packages (from finrl==0.3.6) (0.2.32)\n",
            "Requirement already satisfied: pyfolio<0.10,>=0.9 in /usr/local/lib/python3.10/site-packages (from finrl==0.3.6) (0.9.2)\n",
            "Requirement already satisfied: exchange-calendars<5,>=4 in /usr/local/lib/python3.10/site-packages (from finrl==0.3.6) (4.5)\n",
            "Requirement already satisfied: jqdatasdk<2,>=1 in /usr/local/lib/python3.10/site-packages (from finrl==0.3.6) (1.9.2)\n",
            "Requirement already satisfied: ray[default,tune]<3,>=2 in /usr/local/lib/python3.10/site-packages (from finrl==0.3.6) (2.8.1)\n",
            "Requirement already satisfied: msgpack==1.0.3 in /usr/local/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.26.2)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.7.0)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.26.15)\n",
            "Requirement already satisfied: websockets<11,>=9.0 in /usr/local/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (10.4)\n",
            "Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (6.0)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.31.0)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.1.0)\n",
            "Requirement already satisfied: aiohttp==3.8.2 in /usr/local/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (3.8.2)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (2.1.1)\n",
            "Requirement already satisfied: multidict<6.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (5.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (23.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (4.0.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.4.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.9.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.6) (23.2)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.10/site-packages (from ccxt<4,>=3->finrl==0.3.6) (65.6.3)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.10/site-packages (from ccxt<4,>=3->finrl==0.3.6) (2022.12.7)\n",
            "Requirement already satisfied: aiodns>=1.1.1 in /usr/local/lib/python3.10/site-packages (from ccxt<4,>=3->finrl==0.3.6) (3.1.1)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.10/site-packages (from ccxt<4,>=3->finrl==0.3.6) (40.0.1)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2023.3)\n",
            "Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.10/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (0.3.1)\n",
            "Requirement already satisfied: pyluach in /usr/local/lib/python3.10/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2.2.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2.8.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.16.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /usr/local/lib/python3.10/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.4.50)\n",
            "Requirement already satisfied: pymysql>=0.7.6 in /usr/local/lib/python3.10/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.1.0)\n",
            "Requirement already satisfied: thriftpy2>=0.3.9 in /usr/local/lib/python3.10/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (0.4.17)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (3.8.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (1.11.4)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /usr/local/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.13.0)\n",
            "Requirement already satisfied: ipython>=3.2.3 in /usr/local/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (8.18.1)\n",
            "Requirement already satisfied: pytz>=2014.10 in /usr/local/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (2023.3.post1)\n",
            "Requirement already satisfied: empyrical>=0.5.0 in /usr/local/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.5.5)\n",
            "Requirement already satisfied: cvxpy<2.0.0,>=1.1.19 in /usr/local/lib/python3.10/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (1.4.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (4.23.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (4.20.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (3.13.1)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.3.14)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.59.3)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.19.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.5.5)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.7.0)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.11.3)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (6.4.0)\n",
            "Requirement already satisfied: pydantic<2 in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.10.13)\n",
            "Requirement already satisfied: gpustat>=1.0.0 in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.1.1)\n",
            "Requirement already satisfied: virtualenv<20.21.1,>=20.0.24 in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (20.21.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (14.0.1)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2.6.2.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2023.12.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn<2,>=1->finrl==0.3.6) (3.2.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/site-packages (from scikit-learn<2,>=1->finrl==0.3.6) (1.3.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.1)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.29.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.8.1.78)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.6.1 in /usr/local/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.6.1)\n",
            "Requirement already satisfied: shimmy[atari]~=1.3.0 in /usr/local/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (13.7.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (10.1.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.15.1)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (5.9.6)\n",
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.10/site-packages (from wrds<4,>=3->finrl==0.3.6) (2.9.9)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (3.17.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (0.0.11)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.12.2)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (1.1)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.9.3)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (1.4.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (2.3.10)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (0.26.2)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /usr/local/lib/python3.10/site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.6) (4.4.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.6.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.6) (2.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (1.15.1)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.6.0)\n",
            "Requirement already satisfied: scs>=3.0 in /usr/local/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (3.2.4.post1)\n",
            "Requirement already satisfied: pybind11 in /usr/local/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (2.11.1)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.6.3)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (2.0.12)\n",
            "Requirement already satisfied: pandas-datareader>=0.2 in /usr/local/lib/python3.10/site-packages (from empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.10.0)\n",
            "Requirement already satisfied: nvidia-ml-py>=11.450.129 in /usr/local/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (12.535.133)\n",
            "Requirement already satisfied: blessed>=1.17.1 in /usr/local/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.8.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.0.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/site-packages (from html5lib>=1.1->yfinance<0.3,>=0.2->finrl==0.3.6) (0.5.1)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.0.41)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.9.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.14.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.1.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.19.1)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.17.2)\n",
            "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.6.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.2.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.1.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.46.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.12.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>2->alpaca-trade-api<4,>=3->finrl==0.3.6) (3.4)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/site-packages (from shimmy[atari]~=1.3.0->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.8.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/site-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.6) (3.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.5.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.24.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.1.0)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /usr/local/lib/python3.10/site-packages (from thriftpy2>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6) (3.11)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.18.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.1.0.106)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.1.105)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.2.1)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.1.105)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.12)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.3.101)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/site-packages (from virtualenv<20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6) (0.3.7)\n",
            "Requirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.10/site-packages (from virtualenv<20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6) (3.11.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/site-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (0.0.8)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/site-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (2.3.5)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/site-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (4.1.1.post0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/site-packages (from jsonschema->ray[default,tune]<3,>=2->finrl==0.3.6) (2023.11.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/site-packages (from jsonschema->ray[default,tune]<3,>=2->finrl==0.3.6) (0.13.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/site-packages (from jsonschema->ray[default,tune]<3,>=2->finrl==0.3.6) (0.31.1)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (2.14.0)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.10/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/site-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (6.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (0.2.12)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (2.21)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (1.61.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (5.3.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.8.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.1.2)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/site-packages (from osqp>=0.6.2->cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.1.7.post0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.3)\n",
            "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.2.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "## install required packages\n",
        "!pip install swig\n",
        "!pip install wrds\n",
        "!pip install pyportfolioopt\n",
        "## install finrl library\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import itertools\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from stable_baselines3 import A2C, DDPG, PPO, SAC, TD3\n",
        "from stable_baselines3.common.logger import configure\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "sys.path.append(\"../FinRL\")\n",
        "\n",
        "from finrl import config, config_tickers\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent, TensorboardCallback\n",
        "from finrl.config import (\n",
        "    DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR, INDICATORS, TRAIN_START_DATE, TRAIN_END_DATE,\n",
        "    TEST_START_DATE, TEST_END_DATE, TRADE_START_DATE, TRADE_END_DATE\n",
        ")\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.meta.data_processor import DataProcessor\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
        "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
        "from gymnasium.utils import seeding\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
      ],
      "metadata": {
        "id": "961nWu2ZbaI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ab5d0a4-232e-4a3b-a546-85c23b4b3edb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/site-packages/pyfolio/pos.py:26: UserWarning: Module \"zipline.assets\" not found; mutltipliers will not be applied to position notionals.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class StockEnvMine(gym.Env):\n",
        "\n",
        "    metadata = {\"render.modes\": [\"human\"]}\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            df,\n",
        "            hmax,\n",
        "            initial_amount,\n",
        "            num_stock_shares,\n",
        "            buy_cost_pct,\n",
        "            sell_cost_pct,\n",
        "            state_space,\n",
        "            stock_dim,\n",
        "            tech_indicator_list,\n",
        "            reward_scaling,\n",
        "            action_space,\n",
        "            initial=True,\n",
        "            last_state=[],\n",
        "            turbulence_th=None,\n",
        "            plots=False,\n",
        "            risk_indicator = 'turbulence',\n",
        "            mode=\"\",\n",
        "            model_name=\"\",\n",
        "            iteration=\"\",\n",
        "    ):\n",
        "        self.df = df\n",
        "        self.day = 0\n",
        "        self.data = self.df.loc[self.day, :]\n",
        "        self.hmax = hmax\n",
        "        self.initial_amount = initial_amount\n",
        "        self.num_stock_shares = num_stock_shares\n",
        "        self.buy_cost_pct = buy_cost_pct\n",
        "        self.sell_cost_pct = sell_cost_pct\n",
        "        self.state_space = state_space\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=np.inf, shape=(state_space,))\n",
        "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(action_space,))\n",
        "        self.stock_dim = stock_dim\n",
        "        self.tech_indicator_list = tech_indicator_list\n",
        "        self.reward_scaling = reward_scaling\n",
        "        self.turbulence_th = turbulence_th\n",
        "        self.plots = plots\n",
        "        self.initial = initial\n",
        "        self.terminal = False\n",
        "        self.risk_indicator = risk_indicator\n",
        "        self.state = self.initilize_state()\n",
        "        self.log_every = 1\n",
        "        self.mode = mode\n",
        "        self.model_name = model_name\n",
        "        self.iteration = iteration\n",
        "\n",
        "        self.reward = 0\n",
        "        self.turbulence= 0\n",
        "        self.cost = 0\n",
        "        self.trades = 0\n",
        "        self.episode = 0\n",
        "        self.asset_memory = [self.initial_amount + np.sum(np.array(self.num_stock_shares) * np.array(self.state[1:self.stock_dim+1]))]\n",
        "        self.reward_memory = []\n",
        "        self.actions_memory = []\n",
        "        self.state_memory = ([])\n",
        "        self.date_memory = [self.getDate()]\n",
        "        self.seed()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def initilize_state(self):\n",
        "        if self.initial:\n",
        "            state = ([self.initial_amount] + self.data.close.values.tolist() + self.num_stock_shares + sum((self.data[tech].values.tolist() for tech in self.tech_indicator_list), []))\n",
        "        else:\n",
        "            state = ([self.last_state[0]] + self.data.close.values.tolist() + self.last_state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)] + sum((self.data[tech].values.tolist() for tech in self.tech_indicator_list),[]))\n",
        "        return state\n",
        "\n",
        "    def getDate(self):\n",
        "        return self.data.date.unique()[0]\n",
        "\n",
        "    def render(self, mode=\"human\", close=False):\n",
        "        return self.state\n",
        "\n",
        "    def reset(self, *, seed=None, options=None,):\n",
        "        self.day = 0\n",
        "        self.data = self.df.loc[self.day, :]\n",
        "        self.state = self.initilize_state()\n",
        "        self.asset_memory = [self.initial_amount + np.sum(np.array(self.num_stock_shares) * np.array(self.state[1:self.stock_dim+1]))]\n",
        "        self.turbulence = 0\n",
        "        self.cost = 0\n",
        "        self.trades = 0\n",
        "        self.terminal = False\n",
        "        self.reward_memory = []\n",
        "        self.actions_memory = []\n",
        "        self.date_memory = [self.getDate()]\n",
        "        self.episode += 1\n",
        "        return self.state, {}\n",
        "\n",
        "    def update(self):\n",
        "        state = ([self.state[0]] + self.data.close.values.tolist() + list(self.state[(self.stock_dim + 1) : (2 * self.stock_dim + 1)]) + sum((self.data[tech].values.tolist() for tech in self.tech_indicator_list), []))\n",
        "        return state\n",
        "\n",
        "    def buy(self, index, action):\n",
        "        if self.state[index + 2 * self.stock_dim + 1] != True:\n",
        "            nums_can_buy = self.state[0] // (self.state[index + 1] * (1 + self.buy_cost_pct[index]))\n",
        "            nums = min(nums_can_buy, action)\n",
        "            amount = self.state[index + 1] * (1 + self.buy_cost_pct[index]) * nums\n",
        "            self.state[0] -= amount\n",
        "            self.state[index + self.stock_dim + 1] += nums\n",
        "            self.cost += amount\n",
        "            self.trades += 1\n",
        "        else:\n",
        "            nums = 0\n",
        "        return nums\n",
        "\n",
        "    def Action_Buy(self, index, action):\n",
        "        if self.turbulence_th is None:\n",
        "            nums = self.buy(index, action)\n",
        "        else:\n",
        "            if self.turbulence < self.turbulence_th:\n",
        "                nums = self.buy(index, action)\n",
        "            else:\n",
        "                nums = 0\n",
        "        return nums\n",
        "\n",
        "    def sell(self, index, action):\n",
        "        if self.state[index + 2 * self.stock_dim + 1] != True:\n",
        "            if self.state[index + self.stock_dim + 1] > 0:\n",
        "                nums_can_sell = self.state[index + self.stock_dim + 1]\n",
        "                nums = min(nums_can_sell, abs(action))\n",
        "                amount = self.state[index + 1] * (1 - self.sell_cost_pct[index]) * nums\n",
        "                self.state[0] += amount\n",
        "                self.state[index + self.stock_dim + 1] -= nums\n",
        "                self.cost += self.state[index + 1] * self.sell_cost_pct[index] * nums\n",
        "                self.trades += 1\n",
        "            else:\n",
        "                nums = 0\n",
        "        else:\n",
        "            nums = 0\n",
        "        return nums\n",
        "\n",
        "    def Action_Sell(self, index, action):\n",
        "        if self.turbulence_th is None:\n",
        "            nums = self.sell(index, action)\n",
        "        else:\n",
        "            if self.turbulence < self.turbulence_th:\n",
        "                nums = self.sell(index, action)\n",
        "            else:\n",
        "                if self.state[index + 1] > 0:\n",
        "                    if self.state[index + self.stock_dim + 1] > 0:\n",
        "                        nums = self.state[index + self.stock_dim + 1]\n",
        "                        amount = self.state[index + 1] * (1 - self.sell_cost_pct[index]) * nums\n",
        "                        self.state[0] += amount\n",
        "                        self.state[index + self.stock_dim + 1] = 0\n",
        "                        self.cost += self.state[index + 1] * self.sell_cost_pct[index] * nums\n",
        "                        self.trades += 1\n",
        "                    else:\n",
        "                        nums = 0\n",
        "                else:\n",
        "                    nums = 0\n",
        "        return nums\n",
        "\n",
        "    def makePlot(self):\n",
        "        plt.plot(self.asset_memory, \"r\")\n",
        "        plt.savefig(f\"results/account_value_trade_{self.episode}.png\")\n",
        "        plt.close()\n",
        "\n",
        "    def getDummyEnv(self):\n",
        "        e = DummyVecEnv([lambda: self])\n",
        "        obs = e.reset()\n",
        "        return e, obs\n",
        "\n",
        "    def saveAssetMemory(self):\n",
        "        date_list = self.date_memory\n",
        "        asset_list = self.asset_memory\n",
        "        df_account_value = pd.DataFrame({\"date\": date_list, \"account_value\": asset_list})\n",
        "        return df_account_value\n",
        "\n",
        "    def saveActionMemory(self):\n",
        "        date_list = self.date_memory[:-1]\n",
        "        df_date = pd.DataFrame(date_list)\n",
        "        df_date.columns = [\"date\"]\n",
        "        action_list = self.actions_memory\n",
        "        df_actions = pd.DataFrame(action_list)\n",
        "        df_actions.columns = self.data.tic.values\n",
        "        df_actions.index = df_date.date\n",
        "        return df_actions\n",
        "\n",
        "    def step(self, actions):\n",
        "        self.terminal = (self.day >= len(self.df.index.unique()) - 1 or self.state[0] <= 0)\n",
        "        if self.terminal:\n",
        "            if self.plots:\n",
        "                self.makePlot()\n",
        "            end_asset = self.state[0] + sum(np.array(self.state[1 : (self.stock_dim + 1)]) * np.array(self.state[(self.stock_dim + 1) : (2 * self.stock_dim + 1)]))\n",
        "            df_total_value = pd.DataFrame(self.asset_memory)\n",
        "            total_reward = self.state[0] + sum(np.array(self.state[1 : (self.stock_dim + 1)]) * np.array(self.state[(self.stock_dim + 1) : (2 * self.stock_dim + 1)])) - self.initial_amount\n",
        "            df_total_value.columns = [\"account_value\"]\n",
        "            df_total_value[\"date\"] = self.date_memory\n",
        "            df_total_value[\"daily_return\"] = df_total_value[\"account_value\"].pct_change(1)\n",
        "            if df_total_value[\"daily_return\"].std() != 0:\n",
        "                sharpe = (252 ** 0.5) * df_total_value[\"daily_return\"].mean() / df_total_value[\"daily_return\"].std()\n",
        "            df_rewards = pd.DataFrame(self.reward_memory)\n",
        "            df_rewards.columns = [\"account_rewards\"]\n",
        "            df_rewards[\"date\"] = self.date_memory[:-1]\n",
        "            if self.episode % self.log_every == 0:\n",
        "                print(f\"day: {self.day}, episode: {self.episode}\")\n",
        "                print(f\"begin_total_asset: {self.asset_memory[0]:0.2f}\")\n",
        "                print(f\"end_total_asset: {end_asset:0.2f}\")\n",
        "                print(f\"total_reward: {total_reward:0.2f}\")\n",
        "                print(f\"total_cost: {self.cost:0.2f}\")\n",
        "                print(f\"total_trades: {self.trades}\")\n",
        "                if df_total_value[\"daily_return\"].std() != 0:\n",
        "                    print(f\"Sharpe: {sharpe:0.3f}\")\n",
        "                print(\"=================================\")\n",
        "\n",
        "            if (self.model_name != \"\") and (self.mode != \"\"):\n",
        "                df_total_value.to_csv(\"results/account_value_{}_{}_{}.csv\".format(self.mode, self.model_name, self.iteration),index=False,)\n",
        "                df_rewards.to_csv(\"results/account_rewards_{}_{}_{}.csv\".format(self.mode, self.model_name, self.iteration), index=False,)\n",
        "                plt.plot(self.asset_memory, \"r\")\n",
        "                plt.savefig(\"results/account_value_{}_{}_{}.png\".format(self.mode, self.model_name, self.iteration))\n",
        "                plt.close()\n",
        "            return self.state, self.reward, self.terminal, False, {}\n",
        "        else:\n",
        "            actions = (actions * self.hmax).astype(int)\n",
        "            if self.turbulence_th is not None:\n",
        "                if self.turbulence >= self.turbulence_th:\n",
        "                    actions = np.array([-self.hmax] * self.stock_dim)\n",
        "            begin_asset = self.state[0] + sum(np.array(self.state[1 : (self.stock_dim + 1)]) * np.array(self.state[(self.stock_dim + 1) : (2 * self.stock_dim + 1)]))\n",
        "            sort_action = np.argsort(actions)\n",
        "            sell_index = sort_action[:np.where(actions < 0)[0].shape[0]]\n",
        "            buy_index = sort_action[::-1][:np.where(actions > 0)[0].shape[0]]\n",
        "            for index in sell_index:\n",
        "                actions[index] = self.Action_Sell(index, actions[index]) * (-1)\n",
        "            for index in buy_index:\n",
        "                actions[index] = self.Action_Buy(index, actions[index])\n",
        "            self.actions_memory.append(actions)\n",
        "\n",
        "            self.day += 1\n",
        "            self.data = self.df.loc[self.day, :]\n",
        "            if self.turbulence_th is not None:\n",
        "                self.turbulence = self.data[self.risk_indicator].values[0]\n",
        "            self.state = self.update()\n",
        "            end_total_asset = self.state[0] + sum(np.array(self.state[1 : (self.stock_dim + 1)]) * np.array(self.state[(self.stock_dim + 1) : (2 * self.stock_dim + 1)]))\n",
        "            self.asset_memory.append(end_total_asset)\n",
        "            self.date_memory.append(self.getDate())\n",
        "            # Reward Function\n",
        "            self.reward = end_total_asset - begin_asset\n",
        "            self.reward_memory.append(self.reward)\n",
        "            self.reward = self.reward * self.reward_scaling\n",
        "            self.state_memory.append(self.state)\n",
        "            return self.state, self.reward, self.terminal, False, {}\n"
      ],
      "metadata": {
        "id": "8UZJLGxEbiyx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, model_name, iter_num=0, policy=\"MlpPolicy\", policy_kwargs=None, model_kwargs=None, verbose=1, seed=None, tensorboard_log=None):\n",
        "        self.models = {\"a2c\": A2C, \"ddpg\": DDPG, \"ppo\": PPO, \"a2c_ensemble\": A2C, \"ddpg_ensemble\": DDPG, \"ppo_ensemble\": PPO}\n",
        "        model_kwargs_dict = {x: config.__dict__[f\"{x.upper()}_PARAMS\"] for x in [\"a2c\", \"ddpg\", \"ppo\"]}\n",
        "        model_kwargs_dict_ensemble = {x + \"_ensemble\": config.__dict__[f\"{x.upper()}_PARAMS\"] for x in [\"a2c\", \"ddpg\", \"ppo\"]}\n",
        "        model_kwargs_dict.update(model_kwargs_dict_ensemble)\n",
        "        self.model_name = model_name\n",
        "        self.iter_num = iter_num\n",
        "        if model_kwargs is None:\n",
        "            self.model_kwargs = model_kwargs_dict[model_name]\n",
        "        else:\n",
        "            self.model_kwargs = model_kwargs\n",
        "        self.model = self.models[model_name](policy=policy, env=env, verbose=verbose, seed=seed, tensorboard_log=tensorboard_log, policy_kwargs=policy_kwargs, **self.model_kwargs)\n",
        "\n",
        "    def train(self, total_timesteps=5000):\n",
        "        model = self.model.learn(total_timesteps=total_timesteps, tb_log_name=\"{}_{}\".format(self.model_name, self.iter_num), callback=TensorboardCallback())\n",
        "        model.save(f\"{config.TRAINED_MODEL_DIR}/{self.model_name.upper()}_{total_timesteps // 1000}k_{self.iter_num}\")\n",
        "        return model\n",
        "\n",
        "    def predict(self, env_new, deterministic=True):\n",
        "        env, obs = env_new.getDummyEnv()\n",
        "        account_memory = None\n",
        "        actions_memory = None\n",
        "\n",
        "        env.reset()\n",
        "        max_step = len(env_new.df.index.unique()) - 1\n",
        "\n",
        "        for i in range(max_step + 1):\n",
        "            action, states = self.model.predict(obs, deterministic=deterministic)\n",
        "            obs, rewards, dones, info = env.step(action)\n",
        "            if i == max_step - 1:\n",
        "                account_memory = env.env_method(method_name=\"saveAssetMemory\")\n",
        "                actions_memory = env.env_method(method_name=\"saveActionMemory\")\n",
        "            if dones[0]:\n",
        "                print(\"Finished\")\n",
        "                break\n",
        "        return account_memory[0], actions_memory[0]\n",
        "\n",
        "    def predictLoadFromFile(self, env_new, cwd, deterministic=True):\n",
        "        try:\n",
        "            model = self.model.load(cwd)\n",
        "            print(\"Model loaded from file\")\n",
        "        except BaseException as error:\n",
        "            raise ValueError(f\"Failed to load agent. Error: {str(error)}\") from error\n",
        "\n",
        "        state = env_new.reset()\n",
        "        episode_returns = []\n",
        "        episode_total_assets = [env_new.initial_amount]\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = model.predict(state, deterministic=deterministic)[0]\n",
        "            state, reward, done, _ = env_new.step(action)\n",
        "            episode_total_assets.append(state[0])\n",
        "            episode_return = state[0] / env_new.initial_amount\n",
        "            episode_returns.append(episode_return)\n",
        "        print(f\"Finish Trading. The final amount of money is: {episode_total_assets[-1]}. The total return is: {episode_returns[-1]}\")\n",
        "        return episode_total_assets, episode_returns"
      ],
      "metadata": {
        "id": "6-Nb_xNNb_gX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnsembleAgent:\n",
        "    def __init__(self, df, train_period, val_period, rebalance_window, validation_window, env_args):\n",
        "        self.df = df\n",
        "        self.train_period = train_period\n",
        "        self.val_period = val_period\n",
        "        self.rebalance_window = rebalance_window\n",
        "        self.validation_window = validation_window\n",
        "        self.env_args = env_args\n",
        "        self.unique_trade_date = df[(df.date > val_period[0]) & (df.date <= val_period[1])].date.unique()\n",
        "        self.train_env = None\n",
        "\n",
        "    def val(self, model, val_data, val_env, val_obs):\n",
        "        for _ in range(len(val_data.index.unique())):\n",
        "            action, _states = model.predict(val_obs)\n",
        "            val_obs, rewards, dones, info = val_env.step(action)\n",
        "\n",
        "    def predict(self, model, name, last_state, iter, tur_th, initial):\n",
        "        trade_data = data_split(self.df, start=self.unique_trade_date[iter - self.rebalance_window], end=self.unique_trade_date[iter],)\n",
        "        trade_env = DummyVecEnv([lambda: StockEnvMine(df=trade_data, turbulence_th=tur_th, iteration=iter, mode=\"trade\", model_name=name, last_state=last_state, initial=initial, **self.env_args)])\n",
        "        trade_obs = trade_env.reset()\n",
        "        for i in range(len(trade_data.index.unique())):\n",
        "            action, _ = model.predict(trade_obs)\n",
        "            trade_obs, _, _, _ = trade_env.step(action)\n",
        "            if i == (len(trade_data.index.unique()) - 2):\n",
        "                last_state = trade_env.envs[0].render()\n",
        "\n",
        "        df_last_state = pd.DataFrame({\"last_state\": last_state})\n",
        "        df_last_state.to_csv(f\"results/last_state_{name}_{i}.csv\", index=False)\n",
        "        return last_state\n",
        "\n",
        "    def getSharpe(self, iter, model_name):\n",
        "        df_total_value = pd.read_csv(f\"results/account_value_validation_{model_name}_{iter}.csv\")\n",
        "        if df_total_value[\"daily_return\"].var() == 0:\n",
        "            if df_total_value[\"daily_return\"].mean() > 0:\n",
        "                return np.inf\n",
        "            else:\n",
        "                return 0\n",
        "        else:\n",
        "            return df_total_value[\"daily_return\"].mean() / df_total_value[\"daily_return\"].std() * np.sqrt(4)\n",
        "\n",
        "    def train(self, A2C_kwargs=None, PPO_kwargs=None, DDPG_kwargs=None, timesteps={\"a2c\": 50000, \"ppo\": 50000, \"ddpg\": 50000}):\n",
        "        tell = True\n",
        "        a2c_sharpe = []\n",
        "        ddpg_sharpe = []\n",
        "        ppo_sharpe = []\n",
        "        last_state = []\n",
        "\n",
        "        model_order = []\n",
        "        val_start_date = []\n",
        "        val_end_date = []\n",
        "        iteration_list = []\n",
        "\n",
        "        insample_turbulence = self.df[(self.df.date >= self.train_period[0]) & (self.df.date < self.train_period[1])]\n",
        "        insample_tur_threshold = np.quantile(insample_turbulence.turbulence.values, .90)\n",
        "        for i in range(self.rebalance_window + self.validation_window, len(self.unique_trade_date) + self.rebalance_window + self.validation_window, self.rebalance_window):\n",
        "            val_start = self.unique_trade_date[i - self.rebalance_window - self.validation_window]\n",
        "            if i > len(self.unique_trade_date):\n",
        "              tell = False\n",
        "            if i - self.rebalance_window > len(self.unique_trade_date):\n",
        "              end_index = -1\n",
        "            else:\n",
        "              end_index = i - self.rebalance_window\n",
        "            val_end = self.unique_trade_date[end_index]\n",
        "            # val_start = self.unique_trade_date[i]\n",
        "            # val_end = self.unique_trade_date[i + self.rebalance_window]\n",
        "            val_start_date.append(val_start)\n",
        "            val_end_date.append(val_end)\n",
        "            iteration_list.append(i)\n",
        "            initial = (i - self.rebalance_window - self.validation_window == 0)\n",
        "            end_date = self.df.index[self.df[\"date\"] == self.unique_trade_date[i - self.rebalance_window - self.validation_window]].to_list()[-1]\n",
        "            start_date = end_date - 63 + 1\n",
        "            history_tur_mean = np.mean(self.df.iloc[start_date : (end_date + 1), :].drop_duplicates(subset=[\"date\"]).turbulence.values)\n",
        "            if history_tur_mean > insample_tur_threshold:\n",
        "                tur_threshold = insample_tur_threshold\n",
        "            else:\n",
        "                tur_threshold = np.quantile(insample_turbulence.turbulence.values, 0.99)\n",
        "            print(\"Turbulence threshold: \", tur_threshold)\n",
        "\n",
        "            train = data_split(self.df, start=self.train_period[0], end=self.unique_trade_date[i - self.rebalance_window - self.validation_window],)\n",
        "            validation = data_split(self.df, start=self.unique_trade_date[i - self.rebalance_window - self.validation_window], end=self.unique_trade_date[end_index],)\n",
        "            self.train_env = DummyVecEnv([lambda: StockEnvMine(df=train, **self.env_args)])\n",
        "            print(\"Model training from: {} to {}\".format(self.train_period[0], self.unique_trade_date[i - self.rebalance_window - self.validation_window]))\n",
        "            print(\"A2C Training: \")\n",
        "            agent_a2c = Agent(env=self.train_env, iter_num=i, model_name=\"a2c_ensemble\", model_kwargs=A2C_kwargs)\n",
        "            trained_a2c = agent_a2c.train(total_timesteps=timesteps[\"a2c\"])\n",
        "            agent_a2c.model = trained_a2c\n",
        "            print(\"A2C Validation from {} to {}\".format(val_start, val_end))\n",
        "            val_env_a2c = DummyVecEnv([lambda: StockEnvMine(df=validation, turbulence_th=tur_threshold, iteration=i, mode=\"validation\", model_name=\"a2c_ensemble\", **self.env_args)])\n",
        "            val_obs_a2c = val_env_a2c.reset()\n",
        "            self.val(agent_a2c.model, validation, val_env_a2c, val_obs_a2c)\n",
        "            sharpe_a2c = self.getSharpe(i, model_name=\"a2c_ensemble\")\n",
        "            a2c_sharpe.append(sharpe_a2c)\n",
        "\n",
        "            print(\"DDPG Training: \")\n",
        "            agent_ddpg = Agent(env=self.train_env, iter_num=i, model_name=\"ddpg_ensemble\", model_kwargs=DDPG_kwargs)\n",
        "            trained_ddpg = agent_ddpg.train(total_timesteps=timesteps[\"ddpg\"])\n",
        "            agent_ddpg.model = trained_ddpg\n",
        "            print(\"DDPG Validation from {} to {}\".format(val_start, val_end))\n",
        "            val_env_ddpg = DummyVecEnv([lambda: StockEnvMine(df=validation, turbulence_th=tur_threshold, iteration=i, mode=\"validation\", model_name=\"ddpg_ensemble\", **self.env_args)])\n",
        "            val_obs_ddpg = val_env_ddpg.reset()\n",
        "            self.val(agent_ddpg.model, validation, val_env_ddpg, val_obs_ddpg)\n",
        "            sharpe_ddpg = self.getSharpe(i, model_name=\"ddpg_ensemble\")\n",
        "            ddpg_sharpe.append(sharpe_ddpg)\n",
        "\n",
        "            print(\"PPO Training: \")\n",
        "            agent_ppo = Agent(env=self.train_env, iter_num=i, model_name=\"ppo_ensemble\", model_kwargs=PPO_kwargs)\n",
        "            trained_ppo = agent_ppo.train(total_timesteps=timesteps[\"ppo\"])\n",
        "            agent_ppo.model = trained_ppo\n",
        "            print(\"PPO Validation from {} to {}\".format(val_start, val_end))\n",
        "            val_env_ppo = DummyVecEnv([lambda: StockEnvMine(df=validation, turbulence_th=tur_threshold, iteration=i, mode=\"validation\", model_name=\"ppo_ensemble\", **self.env_args)])\n",
        "            val_obs_ppo = val_env_ppo.reset()\n",
        "            self.val(agent_ppo.model, validation, val_env_ppo, val_obs_ppo)\n",
        "            sharpe_ppo = self.getSharpe(i, model_name=\"ppo_ensemble\")\n",
        "            ppo_sharpe.append(sharpe_ppo)\n",
        "\n",
        "            print(\"Ensemble Model Training: \")\n",
        "            if (sharpe_a2c > sharpe_ppo) & (sharpe_a2c > sharpe_ddpg):\n",
        "                model_order.append(\"a2c\")\n",
        "                model_ensemble = agent_a2c.model\n",
        "            elif (sharpe_ppo >= sharpe_a2c) & (sharpe_ppo >= sharpe_ddpg):\n",
        "                model_order.append(\"ppo\")\n",
        "                model_ensemble = agent_ppo.model\n",
        "            else:\n",
        "                model_order.append(\"ddpg\")\n",
        "                model_ensemble = agent_ddpg.model\n",
        "\n",
        "            if tell:\n",
        "                last_state = self.predict(model=model_ensemble, name=\"ensemble\", last_state=last_state, iter=i, tur_th=tur_threshold, initial=initial)\n",
        "\n",
        "        df_summary = pd.DataFrame({\"iteration\": iteration_list, \"Start Date\": val_start_date, \"End Date\": val_end_date, \"model_order\": model_order, \"a2c_sharpe\": a2c_sharpe, \"ddpg_sharpe\": ddpg_sharpe, \"ppo_sharpe\": ppo_sharpe})\n",
        "        return df_summary\n"
      ],
      "metadata": {
        "id": "75qiD12kcFvh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_START_DATE = '2010-01-01'\n",
        "TRAIN_END_DATE = '2023-01-01'\n",
        "TRADE_START_DATE = '2023-01-01'\n",
        "TRADE_END_DATE = '2023-09-01'"
      ],
      "metadata": {
        "id": "kh54OzxhcMJs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
        "                     end_date = TRADE_END_DATE,\n",
        "                     ticker_list = config_tickers.DOW_30_TICKER).fetch_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnWkeurPdrEk",
        "outputId": "25d93bb4-3b67-4f25-f12c-ad5a4f29cc14"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "Shape of DataFrame:  (100853, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values(['date','tic'],ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "7ewu8gf-dxEW",
        "outputId": "581014af-f5ba-4f27-f90b-b40f3699a5ea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              date        open        high         low       close     volume  \\\n",
              "0       2010-01-04    7.622500    7.660714    7.585000    6.478998  493729600   \n",
              "1       2010-01-04   56.630001   57.869999   56.560001   41.817791    5277400   \n",
              "2       2010-01-04   40.810001   41.099998   40.389999   33.300179    6894300   \n",
              "3       2010-01-04   55.720001   56.389999   54.799999   43.777546    6186700   \n",
              "4       2010-01-04   57.650002   59.189999   57.509998   40.523598    7325600   \n",
              "...            ...         ...         ...         ...         ...        ...   \n",
              "100848  2023-08-31  492.359985  493.820007  476.290009  473.117889    4927700   \n",
              "100849  2023-08-31  245.589996  248.020004  245.449997  245.158066    5532600   \n",
              "100850  2023-08-31   34.849998   35.139999   34.759998   34.248959   24333200   \n",
              "100851  2023-08-31   25.590000   25.760000   25.180000   24.724798   10794500   \n",
              "100852  2023-08-31  161.119995  162.990005  160.960007  162.610001    6527600   \n",
              "\n",
              "         tic  day  \n",
              "0       AAPL    0  \n",
              "1       AMGN    0  \n",
              "2        AXP    0  \n",
              "3         BA    0  \n",
              "4        CAT    0  \n",
              "...      ...  ...  \n",
              "100848   UNH    3  \n",
              "100849     V    3  \n",
              "100850    VZ    3  \n",
              "100851   WBA    3  \n",
              "100852   WMT    3  \n",
              "\n",
              "[100853 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-532827f9-3ec8-4009-9348-aaf0a8eef628\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>tic</th>\n",
              "      <th>day</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>7.622500</td>\n",
              "      <td>7.660714</td>\n",
              "      <td>7.585000</td>\n",
              "      <td>6.478998</td>\n",
              "      <td>493729600</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>56.630001</td>\n",
              "      <td>57.869999</td>\n",
              "      <td>56.560001</td>\n",
              "      <td>41.817791</td>\n",
              "      <td>5277400</td>\n",
              "      <td>AMGN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>40.810001</td>\n",
              "      <td>41.099998</td>\n",
              "      <td>40.389999</td>\n",
              "      <td>33.300179</td>\n",
              "      <td>6894300</td>\n",
              "      <td>AXP</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>55.720001</td>\n",
              "      <td>56.389999</td>\n",
              "      <td>54.799999</td>\n",
              "      <td>43.777546</td>\n",
              "      <td>6186700</td>\n",
              "      <td>BA</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>57.650002</td>\n",
              "      <td>59.189999</td>\n",
              "      <td>57.509998</td>\n",
              "      <td>40.523598</td>\n",
              "      <td>7325600</td>\n",
              "      <td>CAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100848</th>\n",
              "      <td>2023-08-31</td>\n",
              "      <td>492.359985</td>\n",
              "      <td>493.820007</td>\n",
              "      <td>476.290009</td>\n",
              "      <td>473.117889</td>\n",
              "      <td>4927700</td>\n",
              "      <td>UNH</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100849</th>\n",
              "      <td>2023-08-31</td>\n",
              "      <td>245.589996</td>\n",
              "      <td>248.020004</td>\n",
              "      <td>245.449997</td>\n",
              "      <td>245.158066</td>\n",
              "      <td>5532600</td>\n",
              "      <td>V</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100850</th>\n",
              "      <td>2023-08-31</td>\n",
              "      <td>34.849998</td>\n",
              "      <td>35.139999</td>\n",
              "      <td>34.759998</td>\n",
              "      <td>34.248959</td>\n",
              "      <td>24333200</td>\n",
              "      <td>VZ</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100851</th>\n",
              "      <td>2023-08-31</td>\n",
              "      <td>25.590000</td>\n",
              "      <td>25.760000</td>\n",
              "      <td>25.180000</td>\n",
              "      <td>24.724798</td>\n",
              "      <td>10794500</td>\n",
              "      <td>WBA</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100852</th>\n",
              "      <td>2023-08-31</td>\n",
              "      <td>161.119995</td>\n",
              "      <td>162.990005</td>\n",
              "      <td>160.960007</td>\n",
              "      <td>162.610001</td>\n",
              "      <td>6527600</td>\n",
              "      <td>WMT</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100853 rows √ó 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-532827f9-3ec8-4009-9348-aaf0a8eef628')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-532827f9-3ec8-4009-9348-aaf0a8eef628 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-532827f9-3ec8-4009-9348-aaf0a8eef628');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-beda741b-801a-4562-bd5d-883a942c1a40\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-beda741b-801a-4562-bd5d-883a942c1a40')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-beda741b-801a-4562-bd5d-883a942c1a40 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fe = FeatureEngineer(\n",
        "                    use_technical_indicator=True,\n",
        "                    tech_indicator_list = INDICATORS,\n",
        "                    use_vix=True,\n",
        "                    use_turbulence=True,\n",
        "                    user_defined_feature = False)\n",
        "\n",
        "processed = fe.preprocess_data(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1h1Ikl1d2by",
        "outputId": "410c4f47-1da0-4b30-cf15-8e84a4db1d28"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully added technical indicators\n",
            "\r[*********************100%%**********************]  1 of 1 completed\n",
            "Shape of DataFrame:  (3438, 8)\n",
            "Successfully added vix\n",
            "Successfully added turbulence index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_ticker = processed[\"tic\"].unique().tolist()\n",
        "list_date = list(pd.date_range(processed['date'].min(),processed['date'].max()).astype(str))\n",
        "combination = list(itertools.product(list_date,list_ticker))\n",
        "\n",
        "processed_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(processed,on=[\"date\",\"tic\"],how=\"left\")\n",
        "processed_full = processed_full[processed_full['date'].isin(processed['date'])]\n",
        "processed_full = processed_full.sort_values(['date','tic'])\n",
        "\n",
        "processed_full = processed_full.fillna(0)\n",
        "processed_full.sort_values(['date','tic'],ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "8xRxIzKAd9H5",
        "outputId": "ebd9bc38-2d27-4352-c6ac-8cf6817ca39a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             date   tic        open        high         low       close  \\\n",
              "0      2010-01-04  AAPL    7.622500    7.660714    7.585000    6.478998   \n",
              "1      2010-01-04  AMGN   56.630001   57.869999   56.560001   41.817791   \n",
              "2      2010-01-04   AXP   40.810001   41.099998   40.389999   33.300179   \n",
              "3      2010-01-04    BA   55.720001   56.389999   54.799999   43.777546   \n",
              "4      2010-01-04   CAT   57.650002   59.189999   57.509998   40.523598   \n",
              "...           ...   ...         ...         ...         ...         ...   \n",
              "99697  2023-08-30   UNH  493.989990  496.709991  490.290009  487.959290   \n",
              "99698  2023-08-30     V  246.419998  248.229996  246.050003  245.706894   \n",
              "99699  2023-08-30    VZ   34.889999   34.950001   34.549999   33.916065   \n",
              "99700  2023-08-30   WBA   25.629999   25.770000   25.450001   25.008093   \n",
              "99701  2023-08-30   WMT  160.399994  161.289993  159.919998  161.199997   \n",
              "\n",
              "            volume  day      macd     boll_ub     boll_lb      rsi_30  \\\n",
              "0      493729600.0  0.0  0.000000    6.500441    6.468757  100.000000   \n",
              "1        5277400.0  0.0  0.000000    6.500441    6.468757  100.000000   \n",
              "2        6894300.0  0.0  0.000000    6.500441    6.468757  100.000000   \n",
              "3        6186700.0  0.0  0.000000    6.500441    6.468757  100.000000   \n",
              "4        7325600.0  0.0  0.000000    6.500441    6.468757  100.000000   \n",
              "...            ...  ...       ...         ...         ...         ...   \n",
              "99697    2283400.0  2.0 -1.366173  510.656666  480.459697   49.573615   \n",
              "99698    4573300.0  2.0  1.805051  245.028929  235.307539   59.424620   \n",
              "99699   15021400.0  2.0  0.042518   33.674198   31.455613   52.358194   \n",
              "99700    5883700.0  2.0 -1.042668   29.841716   23.664549   34.911829   \n",
              "99701    3655900.0  2.0  0.545948  161.911589  155.852143   59.052621   \n",
              "\n",
              "           cci_30       dx_30  close_30_sma  close_60_sma        vix  \\\n",
              "0       66.666667  100.000000      6.478998      6.478998  20.040001   \n",
              "1       66.666667  100.000000     41.817791     41.817791  20.040001   \n",
              "2       66.666667  100.000000     33.300179     33.300179  20.040001   \n",
              "3       66.666667  100.000000     43.777546     43.777546  20.040001   \n",
              "4       66.666667  100.000000     40.523598     40.523598  20.040001   \n",
              "...           ...         ...           ...           ...        ...   \n",
              "99697 -100.696731    5.966966    497.893921    485.160452  13.880000   \n",
              "99698  241.957153   16.612777    239.150280    235.436313  13.880000   \n",
              "99699  184.246607   24.916800     32.757136     33.562109  13.880000   \n",
              "99700 -117.483859   52.212555     27.544158     28.400918  13.880000   \n",
              "99701  109.591486   26.770831    158.777343    156.531765  13.880000   \n",
              "\n",
              "       turbulence  \n",
              "0        0.000000  \n",
              "1        0.000000  \n",
              "2        0.000000  \n",
              "3        0.000000  \n",
              "4        0.000000  \n",
              "...           ...  \n",
              "99697    7.267617  \n",
              "99698    7.267617  \n",
              "99699    7.267617  \n",
              "99700    7.267617  \n",
              "99701    7.267617  \n",
              "\n",
              "[99702 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-410fa9cf-f5b2-4948-9fa2-a9267feae138\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>tic</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>day</th>\n",
              "      <th>macd</th>\n",
              "      <th>boll_ub</th>\n",
              "      <th>boll_lb</th>\n",
              "      <th>rsi_30</th>\n",
              "      <th>cci_30</th>\n",
              "      <th>dx_30</th>\n",
              "      <th>close_30_sma</th>\n",
              "      <th>close_60_sma</th>\n",
              "      <th>vix</th>\n",
              "      <th>turbulence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>7.622500</td>\n",
              "      <td>7.660714</td>\n",
              "      <td>7.585000</td>\n",
              "      <td>6.478998</td>\n",
              "      <td>493729600.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.500441</td>\n",
              "      <td>6.468757</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>6.478998</td>\n",
              "      <td>6.478998</td>\n",
              "      <td>20.040001</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>AMGN</td>\n",
              "      <td>56.630001</td>\n",
              "      <td>57.869999</td>\n",
              "      <td>56.560001</td>\n",
              "      <td>41.817791</td>\n",
              "      <td>5277400.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.500441</td>\n",
              "      <td>6.468757</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>41.817791</td>\n",
              "      <td>41.817791</td>\n",
              "      <td>20.040001</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>AXP</td>\n",
              "      <td>40.810001</td>\n",
              "      <td>41.099998</td>\n",
              "      <td>40.389999</td>\n",
              "      <td>33.300179</td>\n",
              "      <td>6894300.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.500441</td>\n",
              "      <td>6.468757</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>33.300179</td>\n",
              "      <td>33.300179</td>\n",
              "      <td>20.040001</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>BA</td>\n",
              "      <td>55.720001</td>\n",
              "      <td>56.389999</td>\n",
              "      <td>54.799999</td>\n",
              "      <td>43.777546</td>\n",
              "      <td>6186700.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.500441</td>\n",
              "      <td>6.468757</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>43.777546</td>\n",
              "      <td>43.777546</td>\n",
              "      <td>20.040001</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>CAT</td>\n",
              "      <td>57.650002</td>\n",
              "      <td>59.189999</td>\n",
              "      <td>57.509998</td>\n",
              "      <td>40.523598</td>\n",
              "      <td>7325600.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.500441</td>\n",
              "      <td>6.468757</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>40.523598</td>\n",
              "      <td>40.523598</td>\n",
              "      <td>20.040001</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99697</th>\n",
              "      <td>2023-08-30</td>\n",
              "      <td>UNH</td>\n",
              "      <td>493.989990</td>\n",
              "      <td>496.709991</td>\n",
              "      <td>490.290009</td>\n",
              "      <td>487.959290</td>\n",
              "      <td>2283400.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.366173</td>\n",
              "      <td>510.656666</td>\n",
              "      <td>480.459697</td>\n",
              "      <td>49.573615</td>\n",
              "      <td>-100.696731</td>\n",
              "      <td>5.966966</td>\n",
              "      <td>497.893921</td>\n",
              "      <td>485.160452</td>\n",
              "      <td>13.880000</td>\n",
              "      <td>7.267617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99698</th>\n",
              "      <td>2023-08-30</td>\n",
              "      <td>V</td>\n",
              "      <td>246.419998</td>\n",
              "      <td>248.229996</td>\n",
              "      <td>246.050003</td>\n",
              "      <td>245.706894</td>\n",
              "      <td>4573300.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.805051</td>\n",
              "      <td>245.028929</td>\n",
              "      <td>235.307539</td>\n",
              "      <td>59.424620</td>\n",
              "      <td>241.957153</td>\n",
              "      <td>16.612777</td>\n",
              "      <td>239.150280</td>\n",
              "      <td>235.436313</td>\n",
              "      <td>13.880000</td>\n",
              "      <td>7.267617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99699</th>\n",
              "      <td>2023-08-30</td>\n",
              "      <td>VZ</td>\n",
              "      <td>34.889999</td>\n",
              "      <td>34.950001</td>\n",
              "      <td>34.549999</td>\n",
              "      <td>33.916065</td>\n",
              "      <td>15021400.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.042518</td>\n",
              "      <td>33.674198</td>\n",
              "      <td>31.455613</td>\n",
              "      <td>52.358194</td>\n",
              "      <td>184.246607</td>\n",
              "      <td>24.916800</td>\n",
              "      <td>32.757136</td>\n",
              "      <td>33.562109</td>\n",
              "      <td>13.880000</td>\n",
              "      <td>7.267617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99700</th>\n",
              "      <td>2023-08-30</td>\n",
              "      <td>WBA</td>\n",
              "      <td>25.629999</td>\n",
              "      <td>25.770000</td>\n",
              "      <td>25.450001</td>\n",
              "      <td>25.008093</td>\n",
              "      <td>5883700.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.042668</td>\n",
              "      <td>29.841716</td>\n",
              "      <td>23.664549</td>\n",
              "      <td>34.911829</td>\n",
              "      <td>-117.483859</td>\n",
              "      <td>52.212555</td>\n",
              "      <td>27.544158</td>\n",
              "      <td>28.400918</td>\n",
              "      <td>13.880000</td>\n",
              "      <td>7.267617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99701</th>\n",
              "      <td>2023-08-30</td>\n",
              "      <td>WMT</td>\n",
              "      <td>160.399994</td>\n",
              "      <td>161.289993</td>\n",
              "      <td>159.919998</td>\n",
              "      <td>161.199997</td>\n",
              "      <td>3655900.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.545948</td>\n",
              "      <td>161.911589</td>\n",
              "      <td>155.852143</td>\n",
              "      <td>59.052621</td>\n",
              "      <td>109.591486</td>\n",
              "      <td>26.770831</td>\n",
              "      <td>158.777343</td>\n",
              "      <td>156.531765</td>\n",
              "      <td>13.880000</td>\n",
              "      <td>7.267617</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99702 rows √ó 18 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-410fa9cf-f5b2-4948-9fa2-a9267feae138')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-410fa9cf-f5b2-4948-9fa2-a9267feae138 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-410fa9cf-f5b2-4948-9fa2-a9267feae138');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1043a78d-552f-40c9-a086-2b4f270b241b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1043a78d-552f-40c9-a086-2b4f270b241b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1043a78d-552f-40c9-a086-2b4f270b241b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INDICATORS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucxjzp85eUPt",
        "outputId": "058847c8-81cc-436a-c335-c95061ab263a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['macd',\n",
              " 'boll_ub',\n",
              " 'boll_lb',\n",
              " 'rsi_30',\n",
              " 'cci_30',\n",
              " 'dx_30',\n",
              " 'close_30_sma',\n",
              " 'close_60_sma']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_full = processed_full[processed_full['tic'] != 'DOW']"
      ],
      "metadata": {
        "id": "ldLiiae0e-sA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_dimension = len(processed_full.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVbQ_cakedMB",
        "outputId": "08b8eec6-4c42-4b2c-f98e-b66cd84403fb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stock Dimension: 29, State Space: 291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}"
      ],
      "metadata": {
        "id": "cSZi0ZAxf-P5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble = EnsembleAgent(df=processed_full, train_period=[TRAIN_START_DATE, TRAIN_END_DATE], val_period=[TRADE_START_DATE, TRADE_END_DATE], rebalance_window=63, validation_window=63, env_args=env_kwargs)"
      ],
      "metadata": {
        "id": "_-ynaD1zgeaI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PPO_PARAMS = {\n",
        "    \"n_steps\": 2048,\n",
        "    \"ent_coef\": 0.01,\n",
        "    \"learning_rate\": 0.00025,\n",
        "    \"batch_size\": 128,\n",
        "}"
      ],
      "metadata": {
        "id": "WzBMEnXmgNP1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = ensemble.train(PPO_kwargs=PPO_PARAMS)"
      ],
      "metadata": {
        "id": "U5NS1Zm0hJ0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd1e24d-a73a-4be3-f14d-4604f406caf7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Turbulence threshold:  203.40535487964027\n",
            "Model training from: 2010-01-01 to 2023-01-03\n",
            "A2C Training: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 149         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | 0.642       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -40.6       |\n",
            "|    reward             | -0.15942453 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.11        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 191       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -0.0425   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | -48.3     |\n",
            "|    reward             | 0.9203963 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 5.87      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 212        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0.182      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | -65.9      |\n",
            "|    reward             | -2.6417608 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 6.58       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 224        |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | 36.4       |\n",
            "|    reward             | 0.74926585 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 5.01       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 232        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -92.7      |\n",
            "|    reward             | -2.4571915 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 10         |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 239       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -368      |\n",
            "|    reward             | 10.731653 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 129       |\n",
            "-------------------------------------\n",
            "day: 3271, episode: 1\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4410914.65\n",
            "total_reward: 3410914.65\n",
            "total_cost: 124010764.68\n",
            "total_trades: 76192\n",
            "Sharpe: 0.693\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 243         |\n",
            "|    iterations         | 700         |\n",
            "|    time_elapsed       | 14          |\n",
            "|    total_timesteps    | 3500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | -0.251      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 699         |\n",
            "|    policy_loss        | 97.5        |\n",
            "|    reward             | -0.17071879 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 6.26        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 246       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -121      |\n",
            "|    reward             | 1.9780571 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 9.37      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 249         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 18          |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | 56.5        |\n",
            "|    reward             | -0.31847954 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 3.25        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 251       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -94.9     |\n",
            "|    reward             | 1.2857    |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 6.27      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 253       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | -0.0195   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 196       |\n",
            "|    reward             | 13.368197 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 63.8      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 255      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | -204     |\n",
            "|    reward             | 10.3692  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 68.2     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 257       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 788       |\n",
            "|    reward             | 13.513828 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 445       |\n",
            "-------------------------------------\n",
            "day: 3271, episode: 2\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4880151.05\n",
            "total_reward: 3880151.05\n",
            "total_cost: 49167915.43\n",
            "total_trades: 63887\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 258       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -37.8     |\n",
            "|    reward             | 0.6816825 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.09      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 259        |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 28         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | 134        |\n",
            "|    reward             | 0.64975584 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 12.1       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 260       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 117       |\n",
            "|    reward             | 5.0520725 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 11.6      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 261         |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 32          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | 178         |\n",
            "|    reward             | -0.02875722 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 22.5        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 261        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 34         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -169       |\n",
            "|    reward             | 0.63759226 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 23         |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 260       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -108      |\n",
            "|    reward             | -8.847641 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 105       |\n",
            "-------------------------------------\n",
            "day: 3271, episode: 3\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3480100.65\n",
            "total_reward: 2480100.65\n",
            "total_cost: 24423407.68\n",
            "total_trades: 57213\n",
            "Sharpe: 0.596\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 260         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 38          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | 62          |\n",
            "|    reward             | -0.67461765 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 2.27        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 261        |\n",
            "|    iterations         | 2100       |\n",
            "|    time_elapsed       | 40         |\n",
            "|    total_timesteps    | 10500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0.117      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2099       |\n",
            "|    policy_loss        | 46.6       |\n",
            "|    reward             | 0.17950204 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.97       |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 261          |\n",
            "|    iterations         | 2200         |\n",
            "|    time_elapsed       | 41           |\n",
            "|    total_timesteps    | 11000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -41.6        |\n",
            "|    explained_variance | 1.19e-07     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 2199         |\n",
            "|    policy_loss        | -47.4        |\n",
            "|    reward             | -0.037164286 |\n",
            "|    std                | 1.02         |\n",
            "|    value_loss         | 1.69         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 262       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 43        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | -5.86     |\n",
            "|    reward             | 3.4547615 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.345     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 263       |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 45        |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 18.6      |\n",
            "|    reward             | 1.5291859 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.35      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 263      |\n",
            "|    iterations         | 2500     |\n",
            "|    time_elapsed       | 47       |\n",
            "|    total_timesteps    | 12500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2499     |\n",
            "|    policy_loss        | 122      |\n",
            "|    reward             | 3.153227 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 14.2     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 264        |\n",
            "|    iterations         | 2600       |\n",
            "|    time_elapsed       | 49         |\n",
            "|    total_timesteps    | 13000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2599       |\n",
            "|    policy_loss        | -147       |\n",
            "|    reward             | -13.275718 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 16.3       |\n",
            "--------------------------------------\n",
            "day: 3271, episode: 4\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4263226.95\n",
            "total_reward: 3263226.95\n",
            "total_cost: 40947869.32\n",
            "total_trades: 64229\n",
            "Sharpe: 0.734\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 264        |\n",
            "|    iterations         | 2700       |\n",
            "|    time_elapsed       | 51         |\n",
            "|    total_timesteps    | 13500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2699       |\n",
            "|    policy_loss        | 163        |\n",
            "|    reward             | 0.39334968 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 18.6       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 264        |\n",
            "|    iterations         | 2800       |\n",
            "|    time_elapsed       | 52         |\n",
            "|    total_timesteps    | 14000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2799       |\n",
            "|    policy_loss        | -123       |\n",
            "|    reward             | -0.4913919 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 9.89       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 265      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 54       |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | -453     |\n",
            "|    reward             | 1.145265 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 126      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 265        |\n",
            "|    iterations         | 3000       |\n",
            "|    time_elapsed       | 56         |\n",
            "|    total_timesteps    | 15000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 5.96e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2999       |\n",
            "|    policy_loss        | -20.6      |\n",
            "|    reward             | 0.23518336 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.587      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 266       |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 58        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 35.7      |\n",
            "|    reward             | -8.166215 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 4.95      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 266       |\n",
            "|    iterations         | 3200      |\n",
            "|    time_elapsed       | 60        |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | -0.00582  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | 463       |\n",
            "|    reward             | 1.3618876 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 149       |\n",
            "-------------------------------------\n",
            "day: 3271, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4174307.15\n",
            "total_reward: 3174307.15\n",
            "total_cost: 28311577.43\n",
            "total_trades: 60961\n",
            "Sharpe: 0.672\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 266        |\n",
            "|    iterations         | 3300       |\n",
            "|    time_elapsed       | 61         |\n",
            "|    total_timesteps    | 16500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3299       |\n",
            "|    policy_loss        | -51.7      |\n",
            "|    reward             | 0.75458384 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 2.16       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 267       |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 63        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | -128      |\n",
            "|    reward             | 2.3002012 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 14        |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 267         |\n",
            "|    iterations         | 3500        |\n",
            "|    time_elapsed       | 65          |\n",
            "|    total_timesteps    | 17500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.7       |\n",
            "|    explained_variance | -0.0223     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3499        |\n",
            "|    policy_loss        | -99         |\n",
            "|    reward             | 0.051993687 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 10.4        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 267       |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 67        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | -0.0382   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | 266       |\n",
            "|    reward             | 1.2919618 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 55.3      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 267      |\n",
            "|    iterations         | 3700     |\n",
            "|    time_elapsed       | 69       |\n",
            "|    total_timesteps    | 18500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.7    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3699     |\n",
            "|    policy_loss        | -187     |\n",
            "|    reward             | 2.272403 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 31.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 268      |\n",
            "|    iterations         | 3800     |\n",
            "|    time_elapsed       | 70       |\n",
            "|    total_timesteps    | 19000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3799     |\n",
            "|    policy_loss        | 125      |\n",
            "|    reward             | 5.943505 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 19.9     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 72        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | -1.26e+03 |\n",
            "|    reward             | 21.322767 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.24e+03  |\n",
            "-------------------------------------\n",
            "day: 3271, episode: 6\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4537533.73\n",
            "total_reward: 3537533.73\n",
            "total_cost: 8320484.19\n",
            "total_trades: 58910\n",
            "Sharpe: 0.664\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 74        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | -95.2     |\n",
            "|    reward             | 0.2809126 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 6.05      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 268      |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 76       |\n",
            "|    total_timesteps    | 20500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.8    |\n",
            "|    explained_variance | 0.000392 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | 0.562    |\n",
            "|    reward             | 2.144259 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 4.03     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 4200       |\n",
            "|    time_elapsed       | 78         |\n",
            "|    total_timesteps    | 21000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4199       |\n",
            "|    policy_loss        | -103       |\n",
            "|    reward             | 0.17977427 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 6.34       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 269       |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 79        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | 2.17      |\n",
            "|    reward             | -1.359631 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.406     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 269       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 81        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | -82.1     |\n",
            "|    reward             | 3.3704538 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 7.33      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 269       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 83        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | -293      |\n",
            "|    reward             | 0.6334161 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 53        |\n",
            "-------------------------------------\n",
            "day: 3271, episode: 7\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4470619.29\n",
            "total_reward: 3470619.29\n",
            "total_cost: 9833653.28\n",
            "total_trades: 57084\n",
            "Sharpe: 0.703\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 269      |\n",
            "|    iterations         | 4600     |\n",
            "|    time_elapsed       | 85       |\n",
            "|    total_timesteps    | 23000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4599     |\n",
            "|    policy_loss        | -41.3    |\n",
            "|    reward             | 1.210676 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 3.17     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 269         |\n",
            "|    iterations         | 4700        |\n",
            "|    time_elapsed       | 87          |\n",
            "|    total_timesteps    | 23500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4699        |\n",
            "|    policy_loss        | -111        |\n",
            "|    reward             | -0.48993212 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 9.47        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 4800       |\n",
            "|    time_elapsed       | 89         |\n",
            "|    total_timesteps    | 24000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.9      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4799       |\n",
            "|    policy_loss        | -53.9      |\n",
            "|    reward             | 0.41449803 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 1.94       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 269       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 90        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | -132      |\n",
            "|    reward             | 0.6842793 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 10.4      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 5000       |\n",
            "|    time_elapsed       | 92         |\n",
            "|    total_timesteps    | 25000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4999       |\n",
            "|    policy_loss        | -178       |\n",
            "|    reward             | -3.2131162 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 29.2       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 5100       |\n",
            "|    time_elapsed       | 94         |\n",
            "|    total_timesteps    | 25500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5099       |\n",
            "|    policy_loss        | -447       |\n",
            "|    reward             | -1.9197097 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 138        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 270        |\n",
            "|    iterations         | 5200       |\n",
            "|    time_elapsed       | 96         |\n",
            "|    total_timesteps    | 26000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5199       |\n",
            "|    policy_loss        | -9.83      |\n",
            "|    reward             | -11.076557 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 26.9       |\n",
            "--------------------------------------\n",
            "day: 3271, episode: 8\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5267388.63\n",
            "total_reward: 4267388.63\n",
            "total_cost: 6216561.66\n",
            "total_trades: 58926\n",
            "Sharpe: 0.805\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 98        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | -20.3     |\n",
            "|    reward             | 0.6001311 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 0.242     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 99        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | 0.0136    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | 43.5      |\n",
            "|    reward             | -2.803097 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 1.46      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 270        |\n",
            "|    iterations         | 5500       |\n",
            "|    time_elapsed       | 101        |\n",
            "|    total_timesteps    | 27500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.1      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5499       |\n",
            "|    policy_loss        | -72.6      |\n",
            "|    reward             | 0.35278115 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 3.8        |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 270         |\n",
            "|    iterations         | 5600        |\n",
            "|    time_elapsed       | 103         |\n",
            "|    total_timesteps    | 28000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5599        |\n",
            "|    policy_loss        | -29.4       |\n",
            "|    reward             | -0.20151927 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 0.8         |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 270        |\n",
            "|    iterations         | 5700       |\n",
            "|    time_elapsed       | 105        |\n",
            "|    total_timesteps    | 28500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5699       |\n",
            "|    policy_loss        | -23.3      |\n",
            "|    reward             | 0.96058095 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 2.78       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 270        |\n",
            "|    iterations         | 5800       |\n",
            "|    time_elapsed       | 107        |\n",
            "|    total_timesteps    | 29000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.1      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5799       |\n",
            "|    policy_loss        | 294        |\n",
            "|    reward             | -0.3842309 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 63.8       |\n",
            "--------------------------------------\n",
            "day: 3271, episode: 9\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4890587.99\n",
            "total_reward: 3890587.99\n",
            "total_cost: 5780505.14\n",
            "total_trades: 55856\n",
            "Sharpe: 0.796\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 270         |\n",
            "|    iterations         | 5900        |\n",
            "|    time_elapsed       | 108         |\n",
            "|    total_timesteps    | 29500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.1       |\n",
            "|    explained_variance | -0.24       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5899        |\n",
            "|    policy_loss        | 37.3        |\n",
            "|    reward             | -0.28723848 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 1.05        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 110       |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | -38.8     |\n",
            "|    reward             | 2.2281969 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.01      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 6100      |\n",
            "|    time_elapsed       | 112       |\n",
            "|    total_timesteps    | 30500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6099      |\n",
            "|    policy_loss        | 43.9      |\n",
            "|    reward             | 1.0513672 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.94      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 271        |\n",
            "|    iterations         | 6200       |\n",
            "|    time_elapsed       | 114        |\n",
            "|    total_timesteps    | 31000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6199       |\n",
            "|    policy_loss        | 63.7       |\n",
            "|    reward             | 0.81125945 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 4.18       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 271        |\n",
            "|    iterations         | 6300       |\n",
            "|    time_elapsed       | 116        |\n",
            "|    total_timesteps    | 31500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6299       |\n",
            "|    policy_loss        | -81.8      |\n",
            "|    reward             | -3.2840655 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 6.54       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 271        |\n",
            "|    iterations         | 6400       |\n",
            "|    time_elapsed       | 117        |\n",
            "|    total_timesteps    | 32000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6399       |\n",
            "|    policy_loss        | 21.5       |\n",
            "|    reward             | -7.5631237 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 1.61       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 271       |\n",
            "|    iterations         | 6500      |\n",
            "|    time_elapsed       | 119       |\n",
            "|    total_timesteps    | 32500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6499      |\n",
            "|    policy_loss        | 88.6      |\n",
            "|    reward             | -1.023495 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 15.6      |\n",
            "-------------------------------------\n",
            "day: 3271, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4373227.19\n",
            "total_reward: 3373227.19\n",
            "total_cost: 5860287.08\n",
            "total_trades: 54403\n",
            "Sharpe: 0.764\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 271       |\n",
            "|    iterations         | 6600      |\n",
            "|    time_elapsed       | 121       |\n",
            "|    total_timesteps    | 33000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.2     |\n",
            "|    explained_variance | 0.533     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6599      |\n",
            "|    policy_loss        | 26.6      |\n",
            "|    reward             | 0.5824501 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 0.557     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 271       |\n",
            "|    iterations         | 6700      |\n",
            "|    time_elapsed       | 123       |\n",
            "|    total_timesteps    | 33500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.3     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6699      |\n",
            "|    policy_loss        | 52        |\n",
            "|    reward             | 0.5447078 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.4       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 271       |\n",
            "|    iterations         | 6800      |\n",
            "|    time_elapsed       | 125       |\n",
            "|    total_timesteps    | 34000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6799      |\n",
            "|    policy_loss        | -225      |\n",
            "|    reward             | 1.9911733 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 26        |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 271       |\n",
            "|    iterations         | 6900      |\n",
            "|    time_elapsed       | 126       |\n",
            "|    total_timesteps    | 34500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6899      |\n",
            "|    policy_loss        | -106      |\n",
            "|    reward             | 1.0614792 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 6.75      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 271        |\n",
            "|    iterations         | 7000       |\n",
            "|    time_elapsed       | 128        |\n",
            "|    total_timesteps    | 35000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6999       |\n",
            "|    policy_loss        | 71         |\n",
            "|    reward             | -2.5896223 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 3.46       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 271      |\n",
            "|    iterations         | 7100     |\n",
            "|    time_elapsed       | 130      |\n",
            "|    total_timesteps    | 35500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7099     |\n",
            "|    policy_loss        | 117      |\n",
            "|    reward             | 5.969938 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 11.6     |\n",
            "------------------------------------\n",
            "day: 3271, episode: 11\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4579800.25\n",
            "total_reward: 3579800.25\n",
            "total_cost: 3623119.11\n",
            "total_trades: 54189\n",
            "Sharpe: 0.827\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 271       |\n",
            "|    iterations         | 7200      |\n",
            "|    time_elapsed       | 132       |\n",
            "|    total_timesteps    | 36000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | -20.5     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7199      |\n",
            "|    policy_loss        | -274      |\n",
            "|    reward             | 0.0869588 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 116       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 271        |\n",
            "|    iterations         | 7300       |\n",
            "|    time_elapsed       | 134        |\n",
            "|    total_timesteps    | 36500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7299       |\n",
            "|    policy_loss        | -32.1      |\n",
            "|    reward             | 0.37913734 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 0.783      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 271        |\n",
            "|    iterations         | 7400       |\n",
            "|    time_elapsed       | 136        |\n",
            "|    total_timesteps    | 37000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7399       |\n",
            "|    policy_loss        | 43.4       |\n",
            "|    reward             | -0.5292398 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 1.59       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 271        |\n",
            "|    iterations         | 7500       |\n",
            "|    time_elapsed       | 137        |\n",
            "|    total_timesteps    | 37500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7499       |\n",
            "|    policy_loss        | 25.1       |\n",
            "|    reward             | -0.4796748 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 3.57       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 272         |\n",
            "|    iterations         | 7600        |\n",
            "|    time_elapsed       | 139         |\n",
            "|    total_timesteps    | 38000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7599        |\n",
            "|    policy_loss        | 32.3        |\n",
            "|    reward             | -0.24224502 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 1.3         |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 272       |\n",
            "|    iterations         | 7700      |\n",
            "|    time_elapsed       | 141       |\n",
            "|    total_timesteps    | 38500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7699      |\n",
            "|    policy_loss        | 70.9      |\n",
            "|    reward             | 0.5134042 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 3.83      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 272       |\n",
            "|    iterations         | 7800      |\n",
            "|    time_elapsed       | 143       |\n",
            "|    total_timesteps    | 39000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7799      |\n",
            "|    policy_loss        | 204       |\n",
            "|    reward             | 2.8029957 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 35.3      |\n",
            "-------------------------------------\n",
            "day: 3271, episode: 12\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3911198.11\n",
            "total_reward: 2911198.11\n",
            "total_cost: 2780155.14\n",
            "total_trades: 56723\n",
            "Sharpe: 0.724\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 272         |\n",
            "|    iterations         | 7900        |\n",
            "|    time_elapsed       | 145         |\n",
            "|    total_timesteps    | 39500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.3       |\n",
            "|    explained_variance | 0.275       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7899        |\n",
            "|    policy_loss        | 61.6        |\n",
            "|    reward             | -0.19728847 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 2.44        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 272        |\n",
            "|    iterations         | 8000       |\n",
            "|    time_elapsed       | 146        |\n",
            "|    total_timesteps    | 40000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7999       |\n",
            "|    policy_loss        | 43.3       |\n",
            "|    reward             | 0.22777885 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 1.92       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 272        |\n",
            "|    iterations         | 8100       |\n",
            "|    time_elapsed       | 148        |\n",
            "|    total_timesteps    | 40500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 1.79e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8099       |\n",
            "|    policy_loss        | -23.3      |\n",
            "|    reward             | -1.2510526 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 0.386      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 272        |\n",
            "|    iterations         | 8200       |\n",
            "|    time_elapsed       | 150        |\n",
            "|    total_timesteps    | 41000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8199       |\n",
            "|    policy_loss        | 42.1       |\n",
            "|    reward             | 0.57049733 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 3.97       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 272        |\n",
            "|    iterations         | 8300       |\n",
            "|    time_elapsed       | 152        |\n",
            "|    total_timesteps    | 41500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8299       |\n",
            "|    policy_loss        | -388       |\n",
            "|    reward             | -5.5735135 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 88.6       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 272      |\n",
            "|    iterations         | 8400     |\n",
            "|    time_elapsed       | 154      |\n",
            "|    total_timesteps    | 42000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.4    |\n",
            "|    explained_variance | -0.0354  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8399     |\n",
            "|    policy_loss        | 520      |\n",
            "|    reward             | 3.809187 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 186      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 272       |\n",
            "|    iterations         | 8500      |\n",
            "|    time_elapsed       | 155       |\n",
            "|    total_timesteps    | 42500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8499      |\n",
            "|    policy_loss        | -96       |\n",
            "|    reward             | -6.357473 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 33.4      |\n",
            "-------------------------------------\n",
            "day: 3271, episode: 13\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4086081.77\n",
            "total_reward: 3086081.77\n",
            "total_cost: 2910267.71\n",
            "total_trades: 57892\n",
            "Sharpe: 0.714\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 272       |\n",
            "|    iterations         | 8600      |\n",
            "|    time_elapsed       | 157       |\n",
            "|    total_timesteps    | 43000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8599      |\n",
            "|    policy_loss        | 156       |\n",
            "|    reward             | 1.3876076 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 12.5      |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 272          |\n",
            "|    iterations         | 8700         |\n",
            "|    time_elapsed       | 159          |\n",
            "|    total_timesteps    | 43500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -42.6        |\n",
            "|    explained_variance | 0.679        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 8699         |\n",
            "|    policy_loss        | 35.4         |\n",
            "|    reward             | -0.017147996 |\n",
            "|    std                | 1.05         |\n",
            "|    value_loss         | 0.872        |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 272         |\n",
            "|    iterations         | 8800        |\n",
            "|    time_elapsed       | 161         |\n",
            "|    total_timesteps    | 44000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.6       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8799        |\n",
            "|    policy_loss        | -1.85       |\n",
            "|    reward             | -0.09595613 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 2.2         |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 272       |\n",
            "|    iterations         | 8900      |\n",
            "|    time_elapsed       | 163       |\n",
            "|    total_timesteps    | 44500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8899      |\n",
            "|    policy_loss        | 35.5      |\n",
            "|    reward             | 2.6715708 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 1.41      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 272         |\n",
            "|    iterations         | 9000        |\n",
            "|    time_elapsed       | 165         |\n",
            "|    total_timesteps    | 45000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.6       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8999        |\n",
            "|    policy_loss        | 84.7        |\n",
            "|    reward             | 0.115435496 |\n",
            "|    std                | 1.06        |\n",
            "|    value_loss         | 6.39        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 272        |\n",
            "|    iterations         | 9100       |\n",
            "|    time_elapsed       | 166        |\n",
            "|    total_timesteps    | 45500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9099       |\n",
            "|    policy_loss        | 58.5       |\n",
            "|    reward             | 0.47953987 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 12.9       |\n",
            "--------------------------------------\n",
            "day: 3271, episode: 14\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4514097.96\n",
            "total_reward: 3514097.96\n",
            "total_cost: 2250236.34\n",
            "total_trades: 56521\n",
            "Sharpe: 0.768\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 272          |\n",
            "|    iterations         | 9200         |\n",
            "|    time_elapsed       | 168          |\n",
            "|    total_timesteps    | 46000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -42.6        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 9199         |\n",
            "|    policy_loss        | -31.9        |\n",
            "|    reward             | -0.027610207 |\n",
            "|    std                | 1.05         |\n",
            "|    value_loss         | 0.794        |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 272       |\n",
            "|    iterations         | 9300      |\n",
            "|    time_elapsed       | 170       |\n",
            "|    total_timesteps    | 46500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.6     |\n",
            "|    explained_variance | 0.00406   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9299      |\n",
            "|    policy_loss        | -20.1     |\n",
            "|    reward             | 1.3312659 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.266     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 272         |\n",
            "|    iterations         | 9400        |\n",
            "|    time_elapsed       | 172         |\n",
            "|    total_timesteps    | 47000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.5       |\n",
            "|    explained_variance | -0.343      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9399        |\n",
            "|    policy_loss        | -23.1       |\n",
            "|    reward             | -0.37288043 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 1.1         |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 272       |\n",
            "|    iterations         | 9500      |\n",
            "|    time_elapsed       | 174       |\n",
            "|    total_timesteps    | 47500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9499      |\n",
            "|    policy_loss        | -16.4     |\n",
            "|    reward             | 0.9524501 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 4.67      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 272       |\n",
            "|    iterations         | 9600      |\n",
            "|    time_elapsed       | 176       |\n",
            "|    total_timesteps    | 48000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9599      |\n",
            "|    policy_loss        | 24.1      |\n",
            "|    reward             | 1.5704309 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.508     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 272      |\n",
            "|    iterations         | 9700     |\n",
            "|    time_elapsed       | 177      |\n",
            "|    total_timesteps    | 48500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.5    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9699     |\n",
            "|    policy_loss        | -266     |\n",
            "|    reward             | 2.9279   |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 47.1     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 272       |\n",
            "|    iterations         | 9800      |\n",
            "|    time_elapsed       | 179       |\n",
            "|    total_timesteps    | 49000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9799      |\n",
            "|    policy_loss        | -431      |\n",
            "|    reward             | 2.8842645 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 157       |\n",
            "-------------------------------------\n",
            "day: 3271, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4380021.39\n",
            "total_reward: 3380021.39\n",
            "total_cost: 1979596.60\n",
            "total_trades: 57173\n",
            "Sharpe: 0.767\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 272        |\n",
            "|    iterations         | 9900       |\n",
            "|    time_elapsed       | 181        |\n",
            "|    total_timesteps    | 49500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.5      |\n",
            "|    explained_variance | -0.0193    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9899       |\n",
            "|    policy_loss        | 17.7       |\n",
            "|    reward             | -1.4291338 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 1.76       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 272        |\n",
            "|    iterations         | 10000      |\n",
            "|    time_elapsed       | 183        |\n",
            "|    total_timesteps    | 50000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9999       |\n",
            "|    policy_loss        | -24.9      |\n",
            "|    reward             | 0.37538496 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 1.13       |\n",
            "--------------------------------------\n",
            "A2C Validation from 2023-01-03 to 2023-04-04\n",
            "day: 62, episode: 1\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1039481.04\n",
            "total_reward: 39481.04\n",
            "total_cost: 1327640.39\n",
            "total_trades: 1126\n",
            "Sharpe: 1.201\n",
            "=================================\n",
            "DDPG Training: \n",
            "Using cuda device\n",
            "day: 3271, episode: 17\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3741519.82\n",
            "total_reward: 2741519.82\n",
            "total_cost: 3844211.38\n",
            "total_trades: 49607\n",
            "Sharpe: 0.690\n",
            "=================================\n",
            "day: 3271, episode: 18\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4238051.33\n",
            "total_reward: 3238051.33\n",
            "total_cost: 1339415.11\n",
            "total_trades: 42663\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "day: 3271, episode: 19\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4238051.33\n",
            "total_reward: 3238051.33\n",
            "total_cost: 1339415.11\n",
            "total_trades: 42663\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "day: 3271, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4238051.33\n",
            "total_reward: 3238051.33\n",
            "total_cost: 1339415.11\n",
            "total_trades: 42663\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 4           |\n",
            "|    fps             | 154         |\n",
            "|    time_elapsed    | 84          |\n",
            "|    total_timesteps | 13088       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | -18.5       |\n",
            "|    critic_loss     | 49.1        |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 9816        |\n",
            "|    reward          | -0.12758401 |\n",
            "------------------------------------\n",
            "day: 3271, episode: 21\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4238051.33\n",
            "total_reward: 3238051.33\n",
            "total_cost: 1339415.11\n",
            "total_trades: 42663\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "day: 3271, episode: 22\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4238051.33\n",
            "total_reward: 3238051.33\n",
            "total_cost: 1339415.11\n",
            "total_trades: 42663\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "day: 3271, episode: 23\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4238051.33\n",
            "total_reward: 3238051.33\n",
            "total_cost: 1339415.11\n",
            "total_trades: 42663\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "day: 3271, episode: 24\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4238051.33\n",
            "total_reward: 3238051.33\n",
            "total_cost: 1339415.11\n",
            "total_trades: 42663\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 8           |\n",
            "|    fps             | 138         |\n",
            "|    time_elapsed    | 189         |\n",
            "|    total_timesteps | 26176       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | -14.2       |\n",
            "|    critic_loss     | 6.08        |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 22904       |\n",
            "|    reward          | -0.12758401 |\n",
            "------------------------------------\n",
            "day: 3271, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4238051.33\n",
            "total_reward: 3238051.33\n",
            "total_cost: 1339415.11\n",
            "total_trades: 42663\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "day: 3271, episode: 26\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4238051.33\n",
            "total_reward: 3238051.33\n",
            "total_cost: 1339415.11\n",
            "total_trades: 42663\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "day: 3271, episode: 27\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4238051.33\n",
            "total_reward: 3238051.33\n",
            "total_cost: 1339415.11\n",
            "total_trades: 42663\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "day: 3271, episode: 28\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4238051.33\n",
            "total_reward: 3238051.33\n",
            "total_cost: 1339415.11\n",
            "total_trades: 42663\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 12          |\n",
            "|    fps             | 134         |\n",
            "|    time_elapsed    | 292         |\n",
            "|    total_timesteps | 39264       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | -11.2       |\n",
            "|    critic_loss     | 3.42        |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 35992       |\n",
            "|    reward          | -0.12758401 |\n",
            "------------------------------------\n",
            "day: 3271, episode: 29\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4238051.33\n",
            "total_reward: 3238051.33\n",
            "total_cost: 1339415.11\n",
            "total_trades: 42663\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "day: 3271, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4238051.33\n",
            "total_reward: 3238051.33\n",
            "total_cost: 1339415.11\n",
            "total_trades: 42663\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "day: 3271, episode: 31\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4238051.33\n",
            "total_reward: 3238051.33\n",
            "total_cost: 1339415.11\n",
            "total_trades: 42663\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "day: 3271, episode: 32\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4238051.33\n",
            "total_reward: 3238051.33\n",
            "total_cost: 1339415.11\n",
            "total_trades: 42663\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 16          |\n",
            "|    fps             | 131         |\n",
            "|    time_elapsed    | 396         |\n",
            "|    total_timesteps | 52352       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | -10.4       |\n",
            "|    critic_loss     | 2.43        |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 49080       |\n",
            "|    reward          | -0.12758401 |\n",
            "------------------------------------\n",
            "DDPG Validation from 2023-01-03 to 2023-04-04\n",
            "day: 62, episode: 1\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1100337.43\n",
            "total_reward: 100337.43\n",
            "total_cost: 1280145.20\n",
            "total_trades: 840\n",
            "Sharpe: 2.443\n",
            "=================================\n",
            "PPO Training: \n",
            "Using cuda device\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 352       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 5         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 1.1349074 |\n",
            "----------------------------------\n",
            "day: 3271, episode: 34\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4172932.89\n",
            "total_reward: 3172932.89\n",
            "total_cost: 244976150.90\n",
            "total_trades: 90866\n",
            "Sharpe: 0.729\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 331         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014959059 |\n",
            "|    clip_fraction        | 0.186       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.0075     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.73        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0287     |\n",
            "|    reward               | -2.951479   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 10.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 325         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 18          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018322662 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | 0.00619     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 32.5        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0162     |\n",
            "|    reward               | 0.93367547  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 66.2        |\n",
            "-----------------------------------------\n",
            "day: 3271, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3177987.24\n",
            "total_reward: 2177987.24\n",
            "total_cost: 240756535.57\n",
            "total_trades: 90676\n",
            "Sharpe: 0.601\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 322         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 25          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015204424 |\n",
            "|    clip_fraction        | 0.142       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.00591    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 14.7        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0174     |\n",
            "|    reward               | -1.3448925  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 39          |\n",
            "-----------------------------------------\n",
            "day: 3271, episode: 36\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4214828.48\n",
            "total_reward: 3214828.48\n",
            "total_cost: 236003872.08\n",
            "total_trades: 90096\n",
            "Sharpe: 0.741\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 320         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 31          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022516537 |\n",
            "|    clip_fraction        | 0.226       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | -0.0155     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.11        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.027      |\n",
            "|    reward               | -1.0591279  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 19.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 318         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 38          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017052691 |\n",
            "|    clip_fraction        | 0.187       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | -0.000751   |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 34.6        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0185     |\n",
            "|    reward               | 2.600666    |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 66.6        |\n",
            "-----------------------------------------\n",
            "day: 3271, episode: 37\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3415047.29\n",
            "total_reward: 2415047.29\n",
            "total_cost: 233285244.29\n",
            "total_trades: 89423\n",
            "Sharpe: 0.624\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 317         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 45          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021485658 |\n",
            "|    clip_fraction        | 0.235       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | -0.0284     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.72        |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0209     |\n",
            "|    reward               | 4.914101    |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 21.8        |\n",
            "-----------------------------------------\n",
            "day: 3271, episode: 38\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4604378.03\n",
            "total_reward: 3604378.03\n",
            "total_cost: 237334522.82\n",
            "total_trades: 90403\n",
            "Sharpe: 0.746\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 317         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 51          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019464644 |\n",
            "|    clip_fraction        | 0.212       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | -0.000482   |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 23.5        |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0212     |\n",
            "|    reward               | -0.24590494 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 81.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 316         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 58          |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017562397 |\n",
            "|    clip_fraction        | 0.164       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | -0.00417    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 54.2        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0134     |\n",
            "|    reward               | -0.54897434 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 152         |\n",
            "-----------------------------------------\n",
            "day: 3271, episode: 39\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3729748.21\n",
            "total_reward: 2729748.21\n",
            "total_cost: 234056816.55\n",
            "total_trades: 90075\n",
            "Sharpe: 0.693\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 316         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 64          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023060892 |\n",
            "|    clip_fraction        | 0.246       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | 0.0131      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.27        |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.019      |\n",
            "|    reward               | 1.2929966   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 15.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 316         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 71          |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022950089 |\n",
            "|    clip_fraction        | 0.177       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | 0.00868     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 15.7        |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0192     |\n",
            "|    reward               | 0.940375    |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 62.3        |\n",
            "-----------------------------------------\n",
            "day: 3271, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5692245.73\n",
            "total_reward: 4692245.73\n",
            "total_cost: 235818762.75\n",
            "total_trades: 90005\n",
            "Sharpe: 0.862\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 316         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 77          |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018739432 |\n",
            "|    clip_fraction        | 0.229       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | -0.00491    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 32          |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0146     |\n",
            "|    reward               | -1.6300508  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 75.2        |\n",
            "-----------------------------------------\n",
            "day: 3271, episode: 41\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4649587.72\n",
            "total_reward: 3649587.72\n",
            "total_cost: 231574588.67\n",
            "total_trades: 89894\n",
            "Sharpe: 0.820\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 316         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 84          |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024050161 |\n",
            "|    clip_fraction        | 0.263       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | 0.00253     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 24.3        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0144     |\n",
            "|    reward               | 0.322626    |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 49.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 316         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 90          |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026090287 |\n",
            "|    clip_fraction        | 0.263       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | 0.0153      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 41.1        |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.0168     |\n",
            "|    reward               | -0.6987902  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 75.2        |\n",
            "-----------------------------------------\n",
            "day: 3271, episode: 42\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3695571.30\n",
            "total_reward: 2695571.30\n",
            "total_cost: 220329197.11\n",
            "total_trades: 88880\n",
            "Sharpe: 0.665\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 315         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 97          |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.02251931  |\n",
            "|    clip_fraction        | 0.211       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | -0.0337     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.09        |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0208     |\n",
            "|    reward               | -0.27318937 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 20          |\n",
            "-----------------------------------------\n",
            "day: 3271, episode: 43\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3663176.45\n",
            "total_reward: 2663176.45\n",
            "total_cost: 208222576.46\n",
            "total_trades: 87677\n",
            "Sharpe: 0.686\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 315         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 103         |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023057204 |\n",
            "|    clip_fraction        | 0.173       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | 0.0155      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 26.6        |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0135     |\n",
            "|    reward               | 0.059849244 |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 82.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 313         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 110         |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015771216 |\n",
            "|    clip_fraction        | 0.183       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | 0.0216      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 35.7        |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.013      |\n",
            "|    reward               | -1.4281335  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 56.7        |\n",
            "-----------------------------------------\n",
            "day: 3271, episode: 44\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4254578.37\n",
            "total_reward: 3254578.37\n",
            "total_cost: 219223018.97\n",
            "total_trades: 88506\n",
            "Sharpe: 0.746\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 313         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 117         |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016596492 |\n",
            "|    clip_fraction        | 0.148       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | 0.00507     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.87        |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0145     |\n",
            "|    reward               | 0.7532401   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 21.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 314         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 123         |\n",
            "|    total_timesteps      | 38912       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.02086604  |\n",
            "|    clip_fraction        | 0.203       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42         |\n",
            "|    explained_variance   | 0.0239      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 27.1        |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.019      |\n",
            "|    reward               | -0.45706853 |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 59.4        |\n",
            "-----------------------------------------\n",
            "day: 3271, episode: 45\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4374960.74\n",
            "total_reward: 3374960.74\n",
            "total_cost: 222854405.09\n",
            "total_trades: 88956\n",
            "Sharpe: 0.725\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 313         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 130         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032200437 |\n",
            "|    clip_fraction        | 0.28        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42         |\n",
            "|    explained_variance   | -0.00832    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 19.7        |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.00957    |\n",
            "|    reward               | -0.9901009  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 69.9        |\n",
            "-----------------------------------------\n",
            "day: 3271, episode: 46\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4231545.73\n",
            "total_reward: 3231545.73\n",
            "total_cost: 216659501.50\n",
            "total_trades: 88672\n",
            "Sharpe: 0.761\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 313         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 137         |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028453687 |\n",
            "|    clip_fraction        | 0.311       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | 0.026       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 22.8        |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0211     |\n",
            "|    reward               | -0.15455568 |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 48.3        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 313        |\n",
            "|    iterations           | 22         |\n",
            "|    time_elapsed         | 143        |\n",
            "|    total_timesteps      | 45056      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02280945 |\n",
            "|    clip_fraction        | 0.209      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.1      |\n",
            "|    explained_variance   | 0.0129     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 45.4       |\n",
            "|    n_updates            | 210        |\n",
            "|    policy_gradient_loss | -0.0133    |\n",
            "|    reward               | 1.9429022  |\n",
            "|    std                  | 1.03       |\n",
            "|    value_loss           | 71.5       |\n",
            "----------------------------------------\n",
            "day: 3271, episode: 47\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3713243.55\n",
            "total_reward: 2713243.55\n",
            "total_cost: 223494287.38\n",
            "total_trades: 89096\n",
            "Sharpe: 0.726\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 313         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 150         |\n",
            "|    total_timesteps      | 47104       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021572804 |\n",
            "|    clip_fraction        | 0.231       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | 0.000287    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.79        |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.0177     |\n",
            "|    reward               | -1.4678898  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 15.1        |\n",
            "-----------------------------------------\n",
            "day: 3271, episode: 48\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4153121.83\n",
            "total_reward: 3153121.83\n",
            "total_cost: 217642315.93\n",
            "total_trades: 88092\n",
            "Sharpe: 0.712\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 313         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 156         |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029237803 |\n",
            "|    clip_fraction        | 0.283       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | 0.00702     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 23.7        |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0211     |\n",
            "|    reward               | 0.32121596  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 51          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 313         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 163         |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020289537 |\n",
            "|    clip_fraction        | 0.194       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.3       |\n",
            "|    explained_variance   | -0.0137     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 37          |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.0104     |\n",
            "|    reward               | 3.1378222   |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 79.2        |\n",
            "-----------------------------------------\n",
            "PPO Validation from 2023-01-03 to 2023-04-04\n",
            "day: 62, episode: 1\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 963956.15\n",
            "total_reward: -36043.85\n",
            "total_cost: 7119884.39\n",
            "total_trades: 1586\n",
            "Sharpe: -0.969\n",
            "=================================\n",
            "Ensemble Model Training: \n",
            "day: 62, episode: 1\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1056224.04\n",
            "total_reward: 56224.04\n",
            "total_cost: 1259700.52\n",
            "total_trades: 839\n",
            "Sharpe: 1.810\n",
            "=================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Turbulence threshold:  203.40535487964027\n",
            "Model training from: 2010-01-01 to 2023-04-04\n",
            "A2C Training: \n",
            "Using cuda device\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 268         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -48.8       |\n",
            "|    reward             | -0.06479742 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 2.52        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 266         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | -68.6       |\n",
            "|    reward             | -0.22638185 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 4.94        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 267       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0.0268    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -33.2     |\n",
            "|    reward             | -2.693658 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 3.75      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 266       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0.261     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -31.2     |\n",
            "|    reward             | 0.6275259 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.74      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 266        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -28.5      |\n",
            "|    reward             | 0.59478503 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 5.41       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 266       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0.247     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 141       |\n",
            "|    reward             | 10.685841 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 22.5      |\n",
            "-------------------------------------\n",
            "day: 3334, episode: 1\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4717208.19\n",
            "total_reward: 3717208.19\n",
            "total_cost: 142588959.68\n",
            "total_trades: 77400\n",
            "Sharpe: 0.780\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 266       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -0.205    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 6.21      |\n",
            "|    reward             | -1.513069 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.504     |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 266          |\n",
            "|    iterations         | 800          |\n",
            "|    time_elapsed       | 15           |\n",
            "|    total_timesteps    | 4000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -41.3        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 799          |\n",
            "|    policy_loss        | -15.7        |\n",
            "|    reward             | -0.050200526 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 0.206        |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 266       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -0.177    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 82.2      |\n",
            "|    reward             | 0.8122285 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 6.34      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 266       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 43.4      |\n",
            "|    reward             | 1.2685223 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.37      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 267       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 7.37      |\n",
            "|    reward             | -2.675285 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.394     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 267       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -0.0353   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 106       |\n",
            "|    reward             | 4.5892487 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 11.2      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 267      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 24       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.4    |\n",
            "|    explained_variance | 0.00864  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 159      |\n",
            "|    reward             | 3.639901 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 45.9     |\n",
            "------------------------------------\n",
            "day: 3334, episode: 2\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5155168.38\n",
            "total_reward: 4155168.38\n",
            "total_cost: 96318759.07\n",
            "total_trades: 66419\n",
            "Sharpe: 0.770\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 266       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -0.38     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -69.5     |\n",
            "|    reward             | 1.2028551 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.66      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 266       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0.08      |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -43.4     |\n",
            "|    reward             | 2.3116715 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.18      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 266        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 29         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 85.3       |\n",
            "|    reward             | -2.5828302 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 4.44       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 266         |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 31          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.5       |\n",
            "|    explained_variance | -2.38e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | 7.01        |\n",
            "|    reward             | -0.41366908 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.381       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 267        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 33         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 55.5       |\n",
            "|    reward             | 0.08398078 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 4.27       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 267       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 74.4      |\n",
            "|    reward             | 1.0842656 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 4.91      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 267       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 136       |\n",
            "|    reward             | 0.6810461 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 14.3      |\n",
            "-------------------------------------\n",
            "day: 3334, episode: 3\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2372322.66\n",
            "total_reward: 1372322.66\n",
            "total_cost: 49056523.92\n",
            "total_trades: 63820\n",
            "Sharpe: 0.455\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 267        |\n",
            "|    iterations         | 2100       |\n",
            "|    time_elapsed       | 39         |\n",
            "|    total_timesteps    | 10500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 0.00148    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2099       |\n",
            "|    policy_loss        | -50.3      |\n",
            "|    reward             | -1.0957986 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.38       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 267       |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 41        |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 14.3      |\n",
            "|    reward             | 3.0340216 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.508     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 267        |\n",
            "|    iterations         | 2300       |\n",
            "|    time_elapsed       | 42         |\n",
            "|    total_timesteps    | 11500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 0.361      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2299       |\n",
            "|    policy_loss        | -32.1      |\n",
            "|    reward             | 0.22229278 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.09       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 267       |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 44        |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 64        |\n",
            "|    reward             | -1.369106 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.83      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 267       |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 46        |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | 11.4      |\n",
            "|    reward             | -1.733415 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.368     |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 268          |\n",
            "|    iterations         | 2600         |\n",
            "|    time_elapsed       | 48           |\n",
            "|    total_timesteps    | 13000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -41.6        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 2599         |\n",
            "|    policy_loss        | -88.4        |\n",
            "|    reward             | -0.119819544 |\n",
            "|    std                | 1.02         |\n",
            "|    value_loss         | 5.71         |\n",
            "----------------------------------------\n",
            "day: 3334, episode: 4\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1956024.31\n",
            "total_reward: 956024.31\n",
            "total_cost: 37600402.48\n",
            "total_trades: 58168\n",
            "Sharpe: 0.356\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 268         |\n",
            "|    iterations         | 2700        |\n",
            "|    time_elapsed       | 50          |\n",
            "|    total_timesteps    | 13500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.6       |\n",
            "|    explained_variance | -0.0183     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2699        |\n",
            "|    policy_loss        | -107        |\n",
            "|    reward             | -0.59606904 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 8.74        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 268         |\n",
            "|    iterations         | 2800        |\n",
            "|    time_elapsed       | 52          |\n",
            "|    total_timesteps    | 14000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2799        |\n",
            "|    policy_loss        | 41.5        |\n",
            "|    reward             | -0.12503949 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 2.19        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 54        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | -120      |\n",
            "|    reward             | 0.6618845 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 12.7      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 3000       |\n",
            "|    time_elapsed       | 55         |\n",
            "|    total_timesteps    | 15000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2999       |\n",
            "|    policy_loss        | -39.3      |\n",
            "|    reward             | -0.1994172 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.33       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 3100       |\n",
            "|    time_elapsed       | 57         |\n",
            "|    total_timesteps    | 15500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.8      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3099       |\n",
            "|    policy_loss        | 30.5       |\n",
            "|    reward             | -0.2703286 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.04       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 3200       |\n",
            "|    time_elapsed       | 59         |\n",
            "|    total_timesteps    | 16000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3199       |\n",
            "|    policy_loss        | 95.2       |\n",
            "|    reward             | 0.43411916 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 6.83       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 61        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | -83       |\n",
            "|    reward             | 0.7120288 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 5.4       |\n",
            "-------------------------------------\n",
            "day: 3334, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1912700.41\n",
            "total_reward: 912700.41\n",
            "total_cost: 7193685.25\n",
            "total_trades: 50412\n",
            "Sharpe: 0.378\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 268         |\n",
            "|    iterations         | 3400        |\n",
            "|    time_elapsed       | 63          |\n",
            "|    total_timesteps    | 17000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42         |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3399        |\n",
            "|    policy_loss        | 4.68        |\n",
            "|    reward             | -0.73728174 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.127       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 3500       |\n",
            "|    time_elapsed       | 65         |\n",
            "|    total_timesteps    | 17500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.1      |\n",
            "|    explained_variance | -8.09e-05  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3499       |\n",
            "|    policy_loss        | -19.8      |\n",
            "|    reward             | 0.30717924 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 1.15       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 269       |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 66        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | 29        |\n",
            "|    reward             | 0.9573234 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 1.4       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 3700       |\n",
            "|    time_elapsed       | 68         |\n",
            "|    total_timesteps    | 18500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3699       |\n",
            "|    policy_loss        | -22.6      |\n",
            "|    reward             | 0.32899013 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 2.06       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 269       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 70        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | -114      |\n",
            "|    reward             | 2.7678587 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 11.6      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 269       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 72        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 84.4      |\n",
            "|    reward             | 2.2065027 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 4.7       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 269      |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 74       |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | -211     |\n",
            "|    reward             | 2.828906 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 58.4     |\n",
            "------------------------------------\n",
            "day: 3334, episode: 6\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2885113.97\n",
            "total_reward: 1885113.97\n",
            "total_cost: 4815464.72\n",
            "total_trades: 50018\n",
            "Sharpe: 0.533\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 4100       |\n",
            "|    time_elapsed       | 76         |\n",
            "|    total_timesteps    | 20500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4099       |\n",
            "|    policy_loss        | 78.9       |\n",
            "|    reward             | -2.1541028 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 7.43       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 4200       |\n",
            "|    time_elapsed       | 77         |\n",
            "|    total_timesteps    | 21000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4199       |\n",
            "|    policy_loss        | -49.4      |\n",
            "|    reward             | 0.07846149 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 1.38       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 4300       |\n",
            "|    time_elapsed       | 79         |\n",
            "|    total_timesteps    | 21500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4299       |\n",
            "|    policy_loss        | 2.64       |\n",
            "|    reward             | -3.8024044 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 0.108      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 269       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 81        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | -34.7     |\n",
            "|    reward             | 2.8103595 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.37      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 269      |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 83       |\n",
            "|    total_timesteps    | 22500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | 92.9     |\n",
            "|    reward             | 2.472814 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 7.38     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 4600       |\n",
            "|    time_elapsed       | 85         |\n",
            "|    total_timesteps    | 23000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4599       |\n",
            "|    policy_loss        | -65.6      |\n",
            "|    reward             | -1.4845334 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 6.42       |\n",
            "--------------------------------------\n",
            "day: 3334, episode: 7\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3677649.68\n",
            "total_reward: 2677649.68\n",
            "total_cost: 3397355.97\n",
            "total_trades: 51391\n",
            "Sharpe: 0.698\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 269         |\n",
            "|    iterations         | 4700        |\n",
            "|    time_elapsed       | 87          |\n",
            "|    total_timesteps    | 23500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.4       |\n",
            "|    explained_variance | -0.00085    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4699        |\n",
            "|    policy_loss        | 7.74        |\n",
            "|    reward             | -0.27736324 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 0.119       |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 269          |\n",
            "|    iterations         | 4800         |\n",
            "|    time_elapsed       | 88           |\n",
            "|    total_timesteps    | 24000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -42.4        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 4799         |\n",
            "|    policy_loss        | 2.58         |\n",
            "|    reward             | -0.028752707 |\n",
            "|    std                | 1.05         |\n",
            "|    value_loss         | 2.53         |\n",
            "----------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 269      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 90       |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | 38.4     |\n",
            "|    reward             | 0.845569 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 0.956    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 269       |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 92        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.6     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | -4.33     |\n",
            "|    reward             | 0.4418539 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 1.75      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 269       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 94        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | -93.5     |\n",
            "|    reward             | 3.0482461 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 13.9      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 269      |\n",
            "|    iterations         | 5200     |\n",
            "|    time_elapsed       | 96       |\n",
            "|    total_timesteps    | 26000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5199     |\n",
            "|    policy_loss        | 344      |\n",
            "|    reward             | 0.724955 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 80.4     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 269       |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 98        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | -32.5     |\n",
            "|    reward             | -8.680761 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 1.72      |\n",
            "-------------------------------------\n",
            "day: 3334, episode: 8\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3663824.55\n",
            "total_reward: 2663824.55\n",
            "total_cost: 7948420.00\n",
            "total_trades: 55318\n",
            "Sharpe: 0.666\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 5400       |\n",
            "|    time_elapsed       | 100        |\n",
            "|    total_timesteps    | 27000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5399       |\n",
            "|    policy_loss        | 16.7       |\n",
            "|    reward             | 0.52067924 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 0.265      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 5500       |\n",
            "|    time_elapsed       | 101        |\n",
            "|    total_timesteps    | 27500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5499       |\n",
            "|    policy_loss        | 112        |\n",
            "|    reward             | 0.74533457 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 8.4        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 270        |\n",
            "|    iterations         | 5600       |\n",
            "|    time_elapsed       | 103        |\n",
            "|    total_timesteps    | 28000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5599       |\n",
            "|    policy_loss        | -202       |\n",
            "|    reward             | 0.55830926 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 25.2       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 105       |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | -155      |\n",
            "|    reward             | 0.6053278 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 16.2      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 270        |\n",
            "|    iterations         | 5800       |\n",
            "|    time_elapsed       | 107        |\n",
            "|    total_timesteps    | 29000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.6      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5799       |\n",
            "|    policy_loss        | -122       |\n",
            "|    reward             | 0.48659825 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 12.5       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 270        |\n",
            "|    iterations         | 5900       |\n",
            "|    time_elapsed       | 109        |\n",
            "|    total_timesteps    | 29500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.7      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5899       |\n",
            "|    policy_loss        | 412        |\n",
            "|    reward             | -1.9636228 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 121        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 110       |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.7     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | -4.4      |\n",
            "|    reward             | 3.5092015 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 10.2      |\n",
            "-------------------------------------\n",
            "day: 3334, episode: 9\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3930312.09\n",
            "total_reward: 2930312.09\n",
            "total_cost: 3821250.27\n",
            "total_trades: 55761\n",
            "Sharpe: 0.702\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 270        |\n",
            "|    iterations         | 6100       |\n",
            "|    time_elapsed       | 112        |\n",
            "|    total_timesteps    | 30500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6099       |\n",
            "|    policy_loss        | 14         |\n",
            "|    reward             | 0.26266837 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 2.75       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 270         |\n",
            "|    iterations         | 6200        |\n",
            "|    time_elapsed       | 114         |\n",
            "|    total_timesteps    | 31000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6199        |\n",
            "|    policy_loss        | 26.1        |\n",
            "|    reward             | -0.34810495 |\n",
            "|    std                | 1.06        |\n",
            "|    value_loss         | 0.581       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 270         |\n",
            "|    iterations         | 6300        |\n",
            "|    time_elapsed       | 116         |\n",
            "|    total_timesteps    | 31500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6299        |\n",
            "|    policy_loss        | 191         |\n",
            "|    reward             | -0.19195525 |\n",
            "|    std                | 1.06        |\n",
            "|    value_loss         | 20.8        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 6400      |\n",
            "|    time_elapsed       | 118       |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6399      |\n",
            "|    policy_loss        | -68.3     |\n",
            "|    reward             | 0.3549774 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 4.02      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 6500      |\n",
            "|    time_elapsed       | 120       |\n",
            "|    total_timesteps    | 32500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6499      |\n",
            "|    policy_loss        | 165       |\n",
            "|    reward             | 7.9215574 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 17.4      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 6600      |\n",
            "|    time_elapsed       | 121       |\n",
            "|    total_timesteps    | 33000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6599      |\n",
            "|    policy_loss        | 187       |\n",
            "|    reward             | 2.5556118 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 27        |\n",
            "-------------------------------------\n",
            "day: 3334, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3879131.50\n",
            "total_reward: 2879131.50\n",
            "total_cost: 3592231.13\n",
            "total_trades: 56062\n",
            "Sharpe: 0.686\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 270      |\n",
            "|    iterations         | 6700     |\n",
            "|    time_elapsed       | 123      |\n",
            "|    total_timesteps    | 33500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.9    |\n",
            "|    explained_variance | 2.38e-06 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6699     |\n",
            "|    policy_loss        | 56       |\n",
            "|    reward             | 0.081985 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 1.77     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 270         |\n",
            "|    iterations         | 6800        |\n",
            "|    time_elapsed       | 125         |\n",
            "|    total_timesteps    | 34000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -43         |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6799        |\n",
            "|    policy_loss        | -41.8       |\n",
            "|    reward             | -0.42651147 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 3.08        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 6900      |\n",
            "|    time_elapsed       | 127       |\n",
            "|    total_timesteps    | 34500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6899      |\n",
            "|    policy_loss        | 84.6      |\n",
            "|    reward             | 2.2034905 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 5.56      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 270         |\n",
            "|    iterations         | 7000        |\n",
            "|    time_elapsed       | 129         |\n",
            "|    total_timesteps    | 35000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -43         |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6999        |\n",
            "|    policy_loss        | 51.3        |\n",
            "|    reward             | 0.061153013 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 2.35        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 270        |\n",
            "|    iterations         | 7100       |\n",
            "|    time_elapsed       | 131        |\n",
            "|    total_timesteps    | 35500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.1      |\n",
            "|    explained_variance | -2.38e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7099       |\n",
            "|    policy_loss        | 147        |\n",
            "|    reward             | -3.2341542 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 19.2       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 270      |\n",
            "|    iterations         | 7200     |\n",
            "|    time_elapsed       | 133      |\n",
            "|    total_timesteps    | 36000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -43.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7199     |\n",
            "|    policy_loss        | 62.6     |\n",
            "|    reward             | 1.820741 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 15.1     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 270        |\n",
            "|    iterations         | 7300       |\n",
            "|    time_elapsed       | 134        |\n",
            "|    total_timesteps    | 36500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7299       |\n",
            "|    policy_loss        | -31        |\n",
            "|    reward             | -1.2318616 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 19.1       |\n",
            "--------------------------------------\n",
            "day: 3334, episode: 11\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3900856.54\n",
            "total_reward: 2900856.54\n",
            "total_cost: 3734508.05\n",
            "total_trades: 57345\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 270      |\n",
            "|    iterations         | 7400     |\n",
            "|    time_elapsed       | 136      |\n",
            "|    total_timesteps    | 37000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -43      |\n",
            "|    explained_variance | 5.13e-05 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7399     |\n",
            "|    policy_loss        | -15.1    |\n",
            "|    reward             | 0.537351 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 0.173    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 7500      |\n",
            "|    time_elapsed       | 138       |\n",
            "|    total_timesteps    | 37500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7499      |\n",
            "|    policy_loss        | 34.2      |\n",
            "|    reward             | 2.3066566 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 1.06      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 270        |\n",
            "|    iterations         | 7600       |\n",
            "|    time_elapsed       | 140        |\n",
            "|    total_timesteps    | 38000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7599       |\n",
            "|    policy_loss        | 148        |\n",
            "|    reward             | -0.7156197 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 17.1       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 270        |\n",
            "|    iterations         | 7700       |\n",
            "|    time_elapsed       | 142        |\n",
            "|    total_timesteps    | 38500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7699       |\n",
            "|    policy_loss        | 53.7       |\n",
            "|    reward             | -3.5751152 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 3.53       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 270        |\n",
            "|    iterations         | 7800       |\n",
            "|    time_elapsed       | 144        |\n",
            "|    total_timesteps    | 39000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7799       |\n",
            "|    policy_loss        | -9.65      |\n",
            "|    reward             | 0.48140493 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 5.31       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 7900      |\n",
            "|    time_elapsed       | 145       |\n",
            "|    total_timesteps    | 39500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.2     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7899      |\n",
            "|    policy_loss        | -391      |\n",
            "|    reward             | 1.0665598 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 86.6      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 8000      |\n",
            "|    time_elapsed       | 147       |\n",
            "|    total_timesteps    | 40000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7999      |\n",
            "|    policy_loss        | -271      |\n",
            "|    reward             | -8.965166 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 47.3      |\n",
            "-------------------------------------\n",
            "day: 3334, episode: 12\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4494231.48\n",
            "total_reward: 3494231.48\n",
            "total_cost: 2973125.24\n",
            "total_trades: 59096\n",
            "Sharpe: 0.763\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 8100      |\n",
            "|    time_elapsed       | 149       |\n",
            "|    total_timesteps    | 40500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.2     |\n",
            "|    explained_variance | 3.99e-06  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8099      |\n",
            "|    policy_loss        | -99.5     |\n",
            "|    reward             | 1.7197943 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 8.32      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 8200      |\n",
            "|    time_elapsed       | 151       |\n",
            "|    total_timesteps    | 41000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.2     |\n",
            "|    explained_variance | -0.222    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8199      |\n",
            "|    policy_loss        | 55.6      |\n",
            "|    reward             | 1.0686072 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 1.9       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 270      |\n",
            "|    iterations         | 8300     |\n",
            "|    time_elapsed       | 153      |\n",
            "|    total_timesteps    | 41500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -43.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8299     |\n",
            "|    policy_loss        | -67.2    |\n",
            "|    reward             | 3.088124 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 4.36     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 270         |\n",
            "|    iterations         | 8400        |\n",
            "|    time_elapsed       | 155         |\n",
            "|    total_timesteps    | 42000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -43.2       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8399        |\n",
            "|    policy_loss        | -64.9       |\n",
            "|    reward             | 0.011202985 |\n",
            "|    std                | 1.08        |\n",
            "|    value_loss         | 2.18        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 270      |\n",
            "|    iterations         | 8500     |\n",
            "|    time_elapsed       | 156      |\n",
            "|    total_timesteps    | 42500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -43.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8499     |\n",
            "|    policy_loss        | 205      |\n",
            "|    reward             | 3.673835 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 33.9     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 271       |\n",
            "|    iterations         | 8600      |\n",
            "|    time_elapsed       | 158       |\n",
            "|    total_timesteps    | 43000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8599      |\n",
            "|    policy_loss        | -144      |\n",
            "|    reward             | 0.4727468 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 11.2      |\n",
            "-------------------------------------\n",
            "day: 3334, episode: 13\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4088674.45\n",
            "total_reward: 3088674.45\n",
            "total_cost: 4332354.57\n",
            "total_trades: 57541\n",
            "Sharpe: 0.727\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 8700      |\n",
            "|    time_elapsed       | 160       |\n",
            "|    total_timesteps    | 43500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.2     |\n",
            "|    explained_variance | -0.066    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8699      |\n",
            "|    policy_loss        | 76.7      |\n",
            "|    reward             | 1.3053864 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 3.93      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 270        |\n",
            "|    iterations         | 8800       |\n",
            "|    time_elapsed       | 162        |\n",
            "|    total_timesteps    | 44000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8799       |\n",
            "|    policy_loss        | 73.7       |\n",
            "|    reward             | 0.75226074 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 3.74       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 270         |\n",
            "|    iterations         | 8900        |\n",
            "|    time_elapsed       | 164         |\n",
            "|    total_timesteps    | 44500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -43.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8899        |\n",
            "|    policy_loss        | -169        |\n",
            "|    reward             | -0.28529304 |\n",
            "|    std                | 1.08        |\n",
            "|    value_loss         | 17.5        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 271        |\n",
            "|    iterations         | 9000       |\n",
            "|    time_elapsed       | 166        |\n",
            "|    total_timesteps    | 45000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.5      |\n",
            "|    explained_variance | -0.00182   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8999       |\n",
            "|    policy_loss        | 84.1       |\n",
            "|    reward             | 0.15019439 |\n",
            "|    std                | 1.09       |\n",
            "|    value_loss         | 4.92       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 271       |\n",
            "|    iterations         | 9100      |\n",
            "|    time_elapsed       | 167       |\n",
            "|    total_timesteps    | 45500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9099      |\n",
            "|    policy_loss        | 271       |\n",
            "|    reward             | 3.3857038 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 37.9      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 271        |\n",
            "|    iterations         | 9200       |\n",
            "|    time_elapsed       | 169        |\n",
            "|    total_timesteps    | 46000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.5      |\n",
            "|    explained_variance | -2.38e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9199       |\n",
            "|    policy_loss        | 25.3       |\n",
            "|    reward             | 0.70350534 |\n",
            "|    std                | 1.09       |\n",
            "|    value_loss         | 21.6       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 271       |\n",
            "|    iterations         | 9300      |\n",
            "|    time_elapsed       | 171       |\n",
            "|    total_timesteps    | 46500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9299      |\n",
            "|    policy_loss        | 782       |\n",
            "|    reward             | 5.0693207 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 394       |\n",
            "-------------------------------------\n",
            "day: 3334, episode: 14\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4331984.71\n",
            "total_reward: 3331984.71\n",
            "total_cost: 8971314.71\n",
            "total_trades: 55375\n",
            "Sharpe: 0.754\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 271         |\n",
            "|    iterations         | 9400        |\n",
            "|    time_elapsed       | 173         |\n",
            "|    total_timesteps    | 47000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -43.6       |\n",
            "|    explained_variance | -0.425      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9399        |\n",
            "|    policy_loss        | 39.7        |\n",
            "|    reward             | 0.042802893 |\n",
            "|    std                | 1.09        |\n",
            "|    value_loss         | 2.55        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 271        |\n",
            "|    iterations         | 9500       |\n",
            "|    time_elapsed       | 175        |\n",
            "|    total_timesteps    | 47500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.6      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9499       |\n",
            "|    policy_loss        | -20.7      |\n",
            "|    reward             | -0.2020195 |\n",
            "|    std                | 1.09       |\n",
            "|    value_loss         | 0.35       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 271         |\n",
            "|    iterations         | 9600        |\n",
            "|    time_elapsed       | 177         |\n",
            "|    total_timesteps    | 48000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -43.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9599        |\n",
            "|    policy_loss        | 12.2        |\n",
            "|    reward             | 0.035016596 |\n",
            "|    std                | 1.09        |\n",
            "|    value_loss         | 6.39        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 271      |\n",
            "|    iterations         | 9700     |\n",
            "|    time_elapsed       | 178      |\n",
            "|    total_timesteps    | 48500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -43.6    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9699     |\n",
            "|    policy_loss        | -41.2    |\n",
            "|    reward             | 1.009984 |\n",
            "|    std                | 1.09     |\n",
            "|    value_loss         | 2.65     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 271      |\n",
            "|    iterations         | 9800     |\n",
            "|    time_elapsed       | 180      |\n",
            "|    total_timesteps    | 49000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -43.6    |\n",
            "|    explained_variance | 0.213    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9799     |\n",
            "|    policy_loss        | 8.03     |\n",
            "|    reward             | 4.144518 |\n",
            "|    std                | 1.09     |\n",
            "|    value_loss         | 0.312    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 271       |\n",
            "|    iterations         | 9900      |\n",
            "|    time_elapsed       | 182       |\n",
            "|    total_timesteps    | 49500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.6     |\n",
            "|    explained_variance | -0.0211   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9899      |\n",
            "|    policy_loss        | 226       |\n",
            "|    reward             | -3.063779 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 77.4      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 271        |\n",
            "|    iterations         | 10000      |\n",
            "|    time_elapsed       | 184        |\n",
            "|    total_timesteps    | 50000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.6      |\n",
            "|    explained_variance | -0.0146    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9999       |\n",
            "|    policy_loss        | -316       |\n",
            "|    reward             | -1.9020264 |\n",
            "|    std                | 1.09       |\n",
            "|    value_loss         | 61         |\n",
            "--------------------------------------\n",
            "A2C Validation from 2023-04-04 to 2023-07-06\n",
            "day: 62, episode: 1\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1043611.16\n",
            "total_reward: 43611.16\n",
            "total_cost: 4208745.39\n",
            "total_trades: 1319\n",
            "Sharpe: 1.467\n",
            "=================================\n",
            "DDPG Training: \n",
            "Using cuda device\n",
            "day: 3334, episode: 16\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5114790.73\n",
            "total_reward: 4114790.73\n",
            "total_cost: 3887057.14\n",
            "total_trades: 57812\n",
            "Sharpe: 0.866\n",
            "=================================\n",
            "day: 3334, episode: 17\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5888544.62\n",
            "total_reward: 4888544.62\n",
            "total_cost: 1026324.17\n",
            "total_trades: 50030\n",
            "Sharpe: 0.796\n",
            "=================================\n",
            "day: 3334, episode: 18\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5888544.62\n",
            "total_reward: 4888544.62\n",
            "total_cost: 1026324.17\n",
            "total_trades: 50030\n",
            "Sharpe: 0.796\n",
            "=================================\n",
            "day: 3334, episode: 19\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5888544.62\n",
            "total_reward: 4888544.62\n",
            "total_cost: 1026324.17\n",
            "total_trades: 50030\n",
            "Sharpe: 0.796\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 151       |\n",
            "|    time_elapsed    | 88        |\n",
            "|    total_timesteps | 13340     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 4.54      |\n",
            "|    critic_loss     | 1.81e+03  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 10005     |\n",
            "|    reward          | 6.5665045 |\n",
            "----------------------------------\n",
            "day: 3334, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5888544.62\n",
            "total_reward: 4888544.62\n",
            "total_cost: 1026324.17\n",
            "total_trades: 50030\n",
            "Sharpe: 0.796\n",
            "=================================\n",
            "day: 3334, episode: 21\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5888544.62\n",
            "total_reward: 4888544.62\n",
            "total_cost: 1026324.17\n",
            "total_trades: 50030\n",
            "Sharpe: 0.796\n",
            "=================================\n",
            "day: 3334, episode: 22\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5888544.62\n",
            "total_reward: 4888544.62\n",
            "total_cost: 1026324.17\n",
            "total_trades: 50030\n",
            "Sharpe: 0.796\n",
            "=================================\n",
            "day: 3334, episode: 23\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5888544.62\n",
            "total_reward: 4888544.62\n",
            "total_cost: 1026324.17\n",
            "total_trades: 50030\n",
            "Sharpe: 0.796\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 137       |\n",
            "|    time_elapsed    | 194       |\n",
            "|    total_timesteps | 26680     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -17       |\n",
            "|    critic_loss     | 8.25      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 23345     |\n",
            "|    reward          | 6.5665045 |\n",
            "----------------------------------\n",
            "day: 3334, episode: 24\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5888544.62\n",
            "total_reward: 4888544.62\n",
            "total_cost: 1026324.17\n",
            "total_trades: 50030\n",
            "Sharpe: 0.796\n",
            "=================================\n",
            "day: 3334, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5888544.62\n",
            "total_reward: 4888544.62\n",
            "total_cost: 1026324.17\n",
            "total_trades: 50030\n",
            "Sharpe: 0.796\n",
            "=================================\n",
            "day: 3334, episode: 26\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5888544.62\n",
            "total_reward: 4888544.62\n",
            "total_cost: 1026324.17\n",
            "total_trades: 50030\n",
            "Sharpe: 0.796\n",
            "=================================\n",
            "day: 3334, episode: 27\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5888544.62\n",
            "total_reward: 4888544.62\n",
            "total_cost: 1026324.17\n",
            "total_trades: 50030\n",
            "Sharpe: 0.796\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 133       |\n",
            "|    time_elapsed    | 300       |\n",
            "|    total_timesteps | 40020     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -16.5     |\n",
            "|    critic_loss     | 3.12      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 36685     |\n",
            "|    reward          | 6.5665045 |\n",
            "----------------------------------\n",
            "day: 3334, episode: 28\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5888544.62\n",
            "total_reward: 4888544.62\n",
            "total_cost: 1026324.17\n",
            "total_trades: 50030\n",
            "Sharpe: 0.796\n",
            "=================================\n",
            "day: 3334, episode: 29\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5888544.62\n",
            "total_reward: 4888544.62\n",
            "total_cost: 1026324.17\n",
            "total_trades: 50030\n",
            "Sharpe: 0.796\n",
            "=================================\n",
            "day: 3334, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5888544.62\n",
            "total_reward: 4888544.62\n",
            "total_cost: 1026324.17\n",
            "total_trades: 50030\n",
            "Sharpe: 0.796\n",
            "=================================\n",
            "DDPG Validation from 2023-04-04 to 2023-07-06\n",
            "day: 62, episode: 1\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1034548.58\n",
            "total_reward: 34548.58\n",
            "total_cost: 1015118.89\n",
            "total_trades: 934\n",
            "Sharpe: 1.171\n",
            "=================================\n",
            "PPO Training: \n",
            "Using cuda device\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 348       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 5         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 0.6121982 |\n",
            "----------------------------------\n",
            "day: 3334, episode: 32\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4451578.04\n",
            "total_reward: 3451578.04\n",
            "total_cost: 255408451.51\n",
            "total_trades: 93235\n",
            "Sharpe: 0.749\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 328         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014743565 |\n",
            "|    clip_fraction        | 0.188       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | 0.00293     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.54        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0214     |\n",
            "|    reward               | 0.40830868  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 13.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 323         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 19          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014898388 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | 0.0105      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 43.4        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0213     |\n",
            "|    reward               | -2.1044166  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 74.3        |\n",
            "-----------------------------------------\n",
            "day: 3334, episode: 33\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3672290.54\n",
            "total_reward: 2672290.54\n",
            "total_cost: 238027655.52\n",
            "total_trades: 91569\n",
            "Sharpe: 0.663\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 320         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 25          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012955936 |\n",
            "|    clip_fraction        | 0.147       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.028      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18.7        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0157     |\n",
            "|    reward               | 1.0749849   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 34.5        |\n",
            "-----------------------------------------\n",
            "day: 3334, episode: 34\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3252051.19\n",
            "total_reward: 2252051.19\n",
            "total_cost: 239115752.75\n",
            "total_trades: 91451\n",
            "Sharpe: 0.593\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 318         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 32          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021202445 |\n",
            "|    clip_fraction        | 0.195       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.00292    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 12.6        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0222     |\n",
            "|    reward               | 0.7130731   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 26.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 317         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 38          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017769119 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.00184    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 24.2        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.019      |\n",
            "|    reward               | 6.490825    |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 64.8        |\n",
            "-----------------------------------------\n",
            "day: 3334, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2415698.05\n",
            "total_reward: 1415698.05\n",
            "total_cost: 235096323.61\n",
            "total_trades: 90904\n",
            "Sharpe: 0.448\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 316         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 45          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018800352 |\n",
            "|    clip_fraction        | 0.201       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.0332     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.89        |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.023      |\n",
            "|    reward               | -0.11995705 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 15.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 315         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 51          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018253833 |\n",
            "|    clip_fraction        | 0.223       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.00157     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 29.5        |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0204     |\n",
            "|    reward               | 3.3968112   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 55.8        |\n",
            "-----------------------------------------\n",
            "day: 3334, episode: 36\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3658313.83\n",
            "total_reward: 2658313.83\n",
            "total_cost: 238883209.02\n",
            "total_trades: 91123\n",
            "Sharpe: 0.624\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 315         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 58          |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023618897 |\n",
            "|    clip_fraction        | 0.226       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | -0.00176    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 47.2        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0163     |\n",
            "|    reward               | 0.028162492 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 71.3        |\n",
            "-----------------------------------------\n",
            "day: 3334, episode: 37\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3943950.18\n",
            "total_reward: 2943950.18\n",
            "total_cost: 236160710.42\n",
            "total_trades: 90984\n",
            "Sharpe: 0.693\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 314         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 65          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021327183 |\n",
            "|    clip_fraction        | 0.231       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | -0.000143   |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 17.9        |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0197     |\n",
            "|    reward               | 2.2342513   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 28.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 314         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 71          |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026745573 |\n",
            "|    clip_fraction        | 0.225       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | -0.00531    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 35          |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0197     |\n",
            "|    reward               | -0.12916312 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 59.2        |\n",
            "-----------------------------------------\n",
            "day: 3334, episode: 38\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3398633.86\n",
            "total_reward: 2398633.86\n",
            "total_cost: 196860485.85\n",
            "total_trades: 86829\n",
            "Sharpe: 0.553\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 313        |\n",
            "|    iterations           | 12         |\n",
            "|    time_elapsed         | 78         |\n",
            "|    total_timesteps      | 24576      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01719938 |\n",
            "|    clip_fraction        | 0.194      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.5      |\n",
            "|    explained_variance   | 0.0171     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 6.25       |\n",
            "|    n_updates            | 110        |\n",
            "|    policy_gradient_loss | -0.0258    |\n",
            "|    reward               | 0.71076304 |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 15.3       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 313         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 84          |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019898362 |\n",
            "|    clip_fraction        | 0.174       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | -0.0125     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 35.9        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0139     |\n",
            "|    reward               | 2.166628    |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 69.5        |\n",
            "-----------------------------------------\n",
            "day: 3334, episode: 39\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3164375.50\n",
            "total_reward: 2164375.50\n",
            "total_cost: 205984465.91\n",
            "total_trades: 88035\n",
            "Sharpe: 0.556\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 313         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 91          |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016104849 |\n",
            "|    clip_fraction        | 0.208       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | 0.00844     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 23.2        |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.0131     |\n",
            "|    reward               | 3.275399    |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 55.7        |\n",
            "-----------------------------------------\n",
            "day: 3334, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4919876.36\n",
            "total_reward: 3919876.36\n",
            "total_cost: 226972397.84\n",
            "total_trades: 89809\n",
            "Sharpe: 0.793\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 313         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 98          |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017720066 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | -0.016      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.5         |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0209     |\n",
            "|    reward               | -1.2294132  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 16.2        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 313        |\n",
            "|    iterations           | 16         |\n",
            "|    time_elapsed         | 104        |\n",
            "|    total_timesteps      | 32768      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02379243 |\n",
            "|    clip_fraction        | 0.21       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.7      |\n",
            "|    explained_variance   | -0.00301   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 57.6       |\n",
            "|    n_updates            | 150        |\n",
            "|    policy_gradient_loss | -0.017     |\n",
            "|    reward               | -1.0467855 |\n",
            "|    std                  | 1.02       |\n",
            "|    value_loss           | 79.3       |\n",
            "----------------------------------------\n",
            "day: 3334, episode: 41\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3457308.26\n",
            "total_reward: 2457308.26\n",
            "total_cost: 209030084.14\n",
            "total_trades: 88075\n",
            "Sharpe: 0.638\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 309         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 112         |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017775862 |\n",
            "|    clip_fraction        | 0.202       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | -0.00709    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18.9        |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.014      |\n",
            "|    reward               | -0.5902555  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 33.2        |\n",
            "-----------------------------------------\n",
            "day: 3334, episode: 42\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4207735.13\n",
            "total_reward: 3207735.13\n",
            "total_cost: 220270923.97\n",
            "total_trades: 88661\n",
            "Sharpe: 0.696\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 309         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 118         |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026547603 |\n",
            "|    clip_fraction        | 0.267       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | 0.0406      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.13        |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0242     |\n",
            "|    reward               | 1.4218769   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 22          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 309         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 125         |\n",
            "|    total_timesteps      | 38912       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024444487 |\n",
            "|    clip_fraction        | 0.256       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | 0.00275     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 26.7        |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.0187     |\n",
            "|    reward               | 2.296273    |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 81.2        |\n",
            "-----------------------------------------\n",
            "day: 3334, episode: 43\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5688282.39\n",
            "total_reward: 4688282.39\n",
            "total_cost: 229100000.13\n",
            "total_trades: 89679\n",
            "Sharpe: 0.803\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 309         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 132         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021927513 |\n",
            "|    clip_fraction        | 0.235       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | -0.0459     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.78        |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.0149     |\n",
            "|    reward               | -0.7324831  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 18.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 310         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 138         |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019824766 |\n",
            "|    clip_fraction        | 0.193       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | 0.012       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 48.1        |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0143     |\n",
            "|    reward               | -0.43398404 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 123         |\n",
            "-----------------------------------------\n",
            "day: 3334, episode: 44\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3232767.78\n",
            "total_reward: 2232767.78\n",
            "total_cost: 196924038.45\n",
            "total_trades: 86393\n",
            "Sharpe: 0.568\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 310         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 145         |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017726399 |\n",
            "|    clip_fraction        | 0.216       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | -0.00419    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 14.6        |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.0113     |\n",
            "|    reward               | 0.27711895  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 55          |\n",
            "-----------------------------------------\n",
            "day: 3334, episode: 45\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4218611.47\n",
            "total_reward: 3218611.47\n",
            "total_cost: 202908624.58\n",
            "total_trades: 87465\n",
            "Sharpe: 0.698\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 310         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 151         |\n",
            "|    total_timesteps      | 47104       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033362214 |\n",
            "|    clip_fraction        | 0.285       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | 0.0255      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 14          |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.021      |\n",
            "|    reward               | 1.5562658   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 24.3        |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 310       |\n",
            "|    iterations           | 24        |\n",
            "|    time_elapsed         | 158       |\n",
            "|    total_timesteps      | 49152     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0165361 |\n",
            "|    clip_fraction        | 0.189     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -42       |\n",
            "|    explained_variance   | 0.018     |\n",
            "|    learning_rate        | 0.00025   |\n",
            "|    loss                 | 40.4      |\n",
            "|    n_updates            | 230       |\n",
            "|    policy_gradient_loss | -0.015    |\n",
            "|    reward               | 12.604319 |\n",
            "|    std                  | 1.03      |\n",
            "|    value_loss           | 73.3      |\n",
            "---------------------------------------\n",
            "day: 3334, episode: 46\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4824025.96\n",
            "total_reward: 3824025.96\n",
            "total_cost: 208763781.40\n",
            "total_trades: 87337\n",
            "Sharpe: 0.734\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 310         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 164         |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024290085 |\n",
            "|    clip_fraction        | 0.228       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42         |\n",
            "|    explained_variance   | 0.0229      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 14.3        |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.0196     |\n",
            "|    reward               | 0.26652613  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 24.4        |\n",
            "-----------------------------------------\n",
            "PPO Validation from 2023-04-04 to 2023-07-06\n",
            "day: 62, episode: 1\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 992816.31\n",
            "total_reward: -7183.69\n",
            "total_cost: 7473590.17\n",
            "total_trades: 1601\n",
            "Sharpe: -0.213\n",
            "=================================\n",
            "Ensemble Model Training: \n",
            "Turbulence threshold:  203.40535487964027\n",
            "Model training from: 2010-01-01 to 2023-07-06\n",
            "A2C Training: \n",
            "Using cuda device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 263        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -62.3      |\n",
            "|    reward             | -0.7534478 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 3.14       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 265        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -30.6      |\n",
            "|    reward             | -0.8440286 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.53       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 265        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0.15       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 2.55       |\n",
            "|    reward             | -2.6731253 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 3.25       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 265        |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -0.022     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | 32.9       |\n",
            "|    reward             | -0.2367842 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.88       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 265        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 50         |\n",
            "|    reward             | -0.9528756 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 5.23       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 265       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -241      |\n",
            "|    reward             | 10.565049 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 52.7      |\n",
            "-------------------------------------\n",
            "day: 3397, episode: 1\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4884086.66\n",
            "total_reward: 3884086.66\n",
            "total_cost: 87863708.76\n",
            "total_trades: 75775\n",
            "Sharpe: 0.745\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 266       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0.0834    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -167      |\n",
            "|    reward             | -1.306968 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 23.4      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 266         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 15          |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.5       |\n",
            "|    explained_variance | 0.112       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | -70.2       |\n",
            "|    reward             | -0.29236552 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 3.98        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 266       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 69.2      |\n",
            "|    reward             | 1.9624768 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 4.68      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 266       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0.061     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -31.3     |\n",
            "|    reward             | 2.8167856 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.34      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 266       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -271      |\n",
            "|    reward             | 2.5002744 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 115       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 266        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 22         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | -0.00854   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 361        |\n",
            "|    reward             | -6.0425663 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 90.8       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 266        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 24         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -461       |\n",
            "|    reward             | -11.209039 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 141        |\n",
            "--------------------------------------\n",
            "day: 3397, episode: 2\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3614215.77\n",
            "total_reward: 2614215.77\n",
            "total_cost: 56993509.80\n",
            "total_trades: 72702\n",
            "Sharpe: 0.498\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 267        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 26         |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -6.29      |\n",
            "|    reward             | 0.70195496 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.412      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 267        |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 28         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | -38.9      |\n",
            "|    reward             | 0.90255374 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.49       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 267        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 29         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 25.7       |\n",
            "|    reward             | -3.6612139 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 4.03       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 267       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 37.7      |\n",
            "|    reward             | 2.8724625 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.69      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 267       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -105      |\n",
            "|    reward             | -3.289988 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 24.4      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 267      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.5    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | -851     |\n",
            "|    reward             | 4.395471 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 533      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -343      |\n",
            "|    reward             | -8.064277 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 161       |\n",
            "-------------------------------------\n",
            "day: 3397, episode: 3\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4681604.17\n",
            "total_reward: 3681604.17\n",
            "total_cost: 19924884.53\n",
            "total_trades: 65148\n",
            "Sharpe: 0.528\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 267         |\n",
            "|    iterations         | 2100        |\n",
            "|    time_elapsed       | 39          |\n",
            "|    total_timesteps    | 10500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | -0.192      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2099        |\n",
            "|    policy_loss        | -162        |\n",
            "|    reward             | 0.011645422 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 12.4        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 2200       |\n",
            "|    time_elapsed       | 41         |\n",
            "|    total_timesteps    | 11000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 6.56e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2199       |\n",
            "|    policy_loss        | 122        |\n",
            "|    reward             | 0.44242162 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 11.8       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 2300       |\n",
            "|    time_elapsed       | 42         |\n",
            "|    total_timesteps    | 11500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2299       |\n",
            "|    policy_loss        | -113       |\n",
            "|    reward             | -1.7101494 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 8.27       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 2400       |\n",
            "|    time_elapsed       | 44         |\n",
            "|    total_timesteps    | 12000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2399       |\n",
            "|    policy_loss        | 284        |\n",
            "|    reward             | -1.2279052 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 44.1       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 267        |\n",
            "|    iterations         | 2500       |\n",
            "|    time_elapsed       | 46         |\n",
            "|    total_timesteps    | 12500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2499       |\n",
            "|    policy_loss        | 173        |\n",
            "|    reward             | -2.2182763 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 29.8       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 48        |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | 237       |\n",
            "|    reward             | -6.141307 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 50.5      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 50        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0.015     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | 214       |\n",
            "|    reward             | 1.9893632 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 34.2      |\n",
            "-------------------------------------\n",
            "day: 3397, episode: 4\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5352267.16\n",
            "total_reward: 4352267.16\n",
            "total_cost: 23224520.66\n",
            "total_trades: 63788\n",
            "Sharpe: 0.599\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 2800       |\n",
            "|    time_elapsed       | 52         |\n",
            "|    total_timesteps    | 14000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | -0.0285    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2799       |\n",
            "|    policy_loss        | -115       |\n",
            "|    reward             | -1.5367069 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 30.6       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 267       |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 54        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | -0.0292   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | 20.5      |\n",
            "|    reward             | 2.2601433 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.85      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 267       |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 55        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0.0206    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 5.93      |\n",
            "|    reward             | 7.4100137 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.99      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 57        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 6.21      |\n",
            "|    reward             | 1.9801826 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.531     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 3200       |\n",
            "|    time_elapsed       | 59         |\n",
            "|    total_timesteps    | 16000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | -0.00774   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3199       |\n",
            "|    policy_loss        | 166        |\n",
            "|    reward             | -1.9832431 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 27.2       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 267       |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 61        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | -188      |\n",
            "|    reward             | 6.6625504 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 36.7      |\n",
            "-------------------------------------\n",
            "day: 3397, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 7044881.76\n",
            "total_reward: 6044881.76\n",
            "total_cost: 42885112.82\n",
            "total_trades: 64618\n",
            "Sharpe: 0.757\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 267        |\n",
            "|    iterations         | 3400       |\n",
            "|    time_elapsed       | 63         |\n",
            "|    total_timesteps    | 17000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3399       |\n",
            "|    policy_loss        | -6.94      |\n",
            "|    reward             | 0.28998172 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.0311     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 268         |\n",
            "|    iterations         | 3500        |\n",
            "|    time_elapsed       | 65          |\n",
            "|    total_timesteps    | 17500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3499        |\n",
            "|    policy_loss        | 43          |\n",
            "|    reward             | -0.11018831 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.71        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 67        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | 33.5      |\n",
            "|    reward             | 0.3697431 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.964     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 68        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 72.3      |\n",
            "|    reward             | -4.921395 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 9.14      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 70        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 138       |\n",
            "|    reward             | 0.7418093 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 15        |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 72        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | -504      |\n",
            "|    reward             | 7.1255584 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 149       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 4000       |\n",
            "|    time_elapsed       | 74         |\n",
            "|    total_timesteps    | 20000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3999       |\n",
            "|    policy_loss        | 373        |\n",
            "|    reward             | -2.6286814 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 195        |\n",
            "--------------------------------------\n",
            "day: 3397, episode: 6\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4741078.87\n",
            "total_reward: 3741078.87\n",
            "total_cost: 14441953.44\n",
            "total_trades: 59641\n",
            "Sharpe: 0.587\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 76        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0.0769    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | -91.4     |\n",
            "|    reward             | 2.8751287 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 8.92      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 4200       |\n",
            "|    time_elapsed       | 78         |\n",
            "|    total_timesteps    | 21000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4199       |\n",
            "|    policy_loss        | -34.1      |\n",
            "|    reward             | 0.29547578 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.33       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 80        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | 117       |\n",
            "|    reward             | 1.8770318 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 11.2      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 81        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | -0.00468  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | -97.7     |\n",
            "|    reward             | -2.174767 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 10.9      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 268         |\n",
            "|    iterations         | 4500        |\n",
            "|    time_elapsed       | 83          |\n",
            "|    total_timesteps    | 22500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4499        |\n",
            "|    policy_loss        | 21.6        |\n",
            "|    reward             | -0.08887627 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 3.2         |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 85        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 0.00109   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | -579      |\n",
            "|    reward             | 5.8331323 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 225       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 268      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 87       |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | -360     |\n",
            "|    reward             | 8.936737 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 98       |\n",
            "------------------------------------\n",
            "day: 3397, episode: 7\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3885844.53\n",
            "total_reward: 2885844.53\n",
            "total_cost: 10160458.09\n",
            "total_trades: 63826\n",
            "Sharpe: 0.505\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 268         |\n",
            "|    iterations         | 4800        |\n",
            "|    time_elapsed       | 89          |\n",
            "|    total_timesteps    | 24000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.7       |\n",
            "|    explained_variance | 0.00556     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4799        |\n",
            "|    policy_loss        | -28.2       |\n",
            "|    reward             | -0.82816744 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.536       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 91        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | -0.0185   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | -161      |\n",
            "|    reward             | -0.921064 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 18        |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 268         |\n",
            "|    iterations         | 5000        |\n",
            "|    time_elapsed       | 93          |\n",
            "|    total_timesteps    | 25000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.8       |\n",
            "|    explained_variance | -0.000433   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4999        |\n",
            "|    policy_loss        | -3.13       |\n",
            "|    reward             | -0.18360138 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 14.5        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 95        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | -39.1     |\n",
            "|    reward             | 1.9711217 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.23      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 5200       |\n",
            "|    time_elapsed       | 96         |\n",
            "|    total_timesteps    | 26000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.8      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5199       |\n",
            "|    policy_loss        | -1.34e+03  |\n",
            "|    reward             | -7.4713135 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 1.04e+03   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 98        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | 33.1      |\n",
            "|    reward             | 1.4223063 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 20.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 100       |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | 25.4      |\n",
            "|    reward             | 1.1393536 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 36.1      |\n",
            "-------------------------------------\n",
            "day: 3397, episode: 8\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3913390.03\n",
            "total_reward: 2913390.03\n",
            "total_cost: 8893727.02\n",
            "total_trades: 61869\n",
            "Sharpe: 0.512\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 268         |\n",
            "|    iterations         | 5500        |\n",
            "|    time_elapsed       | 102         |\n",
            "|    total_timesteps    | 27500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42         |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5499        |\n",
            "|    policy_loss        | 30.3        |\n",
            "|    reward             | -0.44048035 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.69        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 5600       |\n",
            "|    time_elapsed       | 104        |\n",
            "|    total_timesteps    | 28000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5599       |\n",
            "|    policy_loss        | 50.8       |\n",
            "|    reward             | -1.0667379 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 1.91       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 5700       |\n",
            "|    time_elapsed       | 106        |\n",
            "|    total_timesteps    | 28500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | -0.00126   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5699       |\n",
            "|    policy_loss        | 52.5       |\n",
            "|    reward             | 0.32056743 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 4.61       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 108       |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | -15.3     |\n",
            "|    reward             | 1.2233318 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 0.342     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 5900       |\n",
            "|    time_elapsed       | 109        |\n",
            "|    total_timesteps    | 29500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5899       |\n",
            "|    policy_loss        | -894       |\n",
            "|    reward             | -0.7900115 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 522        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 111       |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 193       |\n",
            "|    reward             | 10.281568 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 61.8      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 6100       |\n",
            "|    time_elapsed       | 113        |\n",
            "|    total_timesteps    | 30500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6099       |\n",
            "|    policy_loss        | -35        |\n",
            "|    reward             | 0.54763246 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 17.3       |\n",
            "--------------------------------------\n",
            "day: 3397, episode: 9\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4973213.58\n",
            "total_reward: 3973213.58\n",
            "total_cost: 3828781.75\n",
            "total_trades: 65300\n",
            "Sharpe: 0.642\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 6200       |\n",
            "|    time_elapsed       | 115        |\n",
            "|    total_timesteps    | 31000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6199       |\n",
            "|    policy_loss        | 11.3       |\n",
            "|    reward             | 0.47621047 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 8.06       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 6300       |\n",
            "|    time_elapsed       | 117        |\n",
            "|    total_timesteps    | 31500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.1      |\n",
            "|    explained_variance | 0.00329    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6299       |\n",
            "|    policy_loss        | -73        |\n",
            "|    reward             | -3.2447982 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 6.84       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 6400      |\n",
            "|    time_elapsed       | 119       |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6399      |\n",
            "|    policy_loss        | -4.48     |\n",
            "|    reward             | -7.346204 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 4.8       |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 268         |\n",
            "|    iterations         | 6500        |\n",
            "|    time_elapsed       | 121         |\n",
            "|    total_timesteps    | 32500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.2       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6499        |\n",
            "|    policy_loss        | -56.9       |\n",
            "|    reward             | -0.40032566 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 2.69        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 6600      |\n",
            "|    time_elapsed       | 122       |\n",
            "|    total_timesteps    | 33000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6599      |\n",
            "|    policy_loss        | -494      |\n",
            "|    reward             | 7.4721394 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 229       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 6700      |\n",
            "|    time_elapsed       | 124       |\n",
            "|    total_timesteps    | 33500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6699      |\n",
            "|    policy_loss        | -221      |\n",
            "|    reward             | 0.3195265 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 33.9      |\n",
            "-------------------------------------\n",
            "day: 3397, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 7166542.06\n",
            "total_reward: 6166542.06\n",
            "total_cost: 2962519.39\n",
            "total_trades: 66596\n",
            "Sharpe: 0.779\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 6800       |\n",
            "|    time_elapsed       | 126        |\n",
            "|    total_timesteps    | 34000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6799       |\n",
            "|    policy_loss        | -54.9      |\n",
            "|    reward             | 0.74817264 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 3.08       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 6900       |\n",
            "|    time_elapsed       | 128        |\n",
            "|    total_timesteps    | 34500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6899       |\n",
            "|    policy_loss        | 41.9       |\n",
            "|    reward             | -0.4094056 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 1.27       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 7000       |\n",
            "|    time_elapsed       | 130        |\n",
            "|    total_timesteps    | 35000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6999       |\n",
            "|    policy_loss        | 25         |\n",
            "|    reward             | -2.6352117 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 2.52       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 268         |\n",
            "|    iterations         | 7100        |\n",
            "|    time_elapsed       | 132         |\n",
            "|    total_timesteps    | 35500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.2       |\n",
            "|    explained_variance | -3.77e-05   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7099        |\n",
            "|    policy_loss        | -281        |\n",
            "|    reward             | -0.27500412 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 74.5        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 268      |\n",
            "|    iterations         | 7200     |\n",
            "|    time_elapsed       | 133      |\n",
            "|    total_timesteps    | 36000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7199     |\n",
            "|    policy_loss        | 128      |\n",
            "|    reward             | 4.735357 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 9.54     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 7300      |\n",
            "|    time_elapsed       | 135       |\n",
            "|    total_timesteps    | 36500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7299      |\n",
            "|    policy_loss        | -15.2     |\n",
            "|    reward             | 1.2922151 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.36      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 7400       |\n",
            "|    time_elapsed       | 137        |\n",
            "|    total_timesteps    | 37000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.3      |\n",
            "|    explained_variance | 1.13e-05   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7399       |\n",
            "|    policy_loss        | 566        |\n",
            "|    reward             | -2.5084426 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 234        |\n",
            "--------------------------------------\n",
            "day: 3397, episode: 11\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6659284.88\n",
            "total_reward: 5659284.88\n",
            "total_cost: 2848323.24\n",
            "total_trades: 66737\n",
            "Sharpe: 0.779\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 7500       |\n",
            "|    time_elapsed       | 139        |\n",
            "|    total_timesteps    | 37500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7499       |\n",
            "|    policy_loss        | -71.3      |\n",
            "|    reward             | -3.5057492 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 3.18       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 7600       |\n",
            "|    time_elapsed       | 141        |\n",
            "|    total_timesteps    | 38000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7599       |\n",
            "|    policy_loss        | 68.6       |\n",
            "|    reward             | -2.0402973 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 3.1        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 7700      |\n",
            "|    time_elapsed       | 143       |\n",
            "|    total_timesteps    | 38500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7699      |\n",
            "|    policy_loss        | -157      |\n",
            "|    reward             | 0.4223086 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 13.5      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 268         |\n",
            "|    iterations         | 7800        |\n",
            "|    time_elapsed       | 145         |\n",
            "|    total_timesteps    | 39000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7799        |\n",
            "|    policy_loss        | 85.6        |\n",
            "|    reward             | -0.27307692 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 5.22        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 7900      |\n",
            "|    time_elapsed       | 146       |\n",
            "|    total_timesteps    | 39500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7899      |\n",
            "|    policy_loss        | 17.1      |\n",
            "|    reward             | 0.2607883 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 13.3      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 268      |\n",
            "|    iterations         | 8000     |\n",
            "|    time_elapsed       | 148      |\n",
            "|    total_timesteps    | 40000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.5    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7999     |\n",
            "|    policy_loss        | 306      |\n",
            "|    reward             | 4.585264 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 102      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 269       |\n",
            "|    iterations         | 8100      |\n",
            "|    time_elapsed       | 150       |\n",
            "|    total_timesteps    | 40500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8099      |\n",
            "|    policy_loss        | -1.06e+03 |\n",
            "|    reward             | 19.709955 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 775       |\n",
            "-------------------------------------\n",
            "day: 3397, episode: 12\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 7348894.70\n",
            "total_reward: 6348894.70\n",
            "total_cost: 2941276.19\n",
            "total_trades: 69127\n",
            "Sharpe: 0.813\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 269         |\n",
            "|    iterations         | 8200        |\n",
            "|    time_elapsed       | 152         |\n",
            "|    total_timesteps    | 41000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8199        |\n",
            "|    policy_loss        | -105        |\n",
            "|    reward             | -0.24898332 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 8.06        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 268        |\n",
            "|    iterations         | 8300       |\n",
            "|    time_elapsed       | 154        |\n",
            "|    total_timesteps    | 41500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.5      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8299       |\n",
            "|    policy_loss        | -133       |\n",
            "|    reward             | 0.50832087 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 13.3       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 268       |\n",
            "|    iterations         | 8400      |\n",
            "|    time_elapsed       | 156       |\n",
            "|    total_timesteps    | 42000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8399      |\n",
            "|    policy_loss        | 58.7      |\n",
            "|    reward             | 0.4220735 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 3.43      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 269      |\n",
            "|    iterations         | 8500     |\n",
            "|    time_elapsed       | 157      |\n",
            "|    total_timesteps    | 42500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.7    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8499     |\n",
            "|    policy_loss        | -46.4    |\n",
            "|    reward             | 5.005425 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 1.58     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 269       |\n",
            "|    iterations         | 8600      |\n",
            "|    time_elapsed       | 159       |\n",
            "|    total_timesteps    | 43000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.7     |\n",
            "|    explained_variance | -0.00154  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8599      |\n",
            "|    policy_loss        | -128      |\n",
            "|    reward             | 4.6846185 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 33.4      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 8700       |\n",
            "|    time_elapsed       | 161        |\n",
            "|    total_timesteps    | 43500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8699       |\n",
            "|    policy_loss        | -267       |\n",
            "|    reward             | -16.864367 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 54.1       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 269      |\n",
            "|    iterations         | 8800     |\n",
            "|    time_elapsed       | 163      |\n",
            "|    total_timesteps    | 44000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8799     |\n",
            "|    policy_loss        | 513      |\n",
            "|    reward             | 5.582093 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 170      |\n",
            "------------------------------------\n",
            "day: 3397, episode: 13\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6268102.62\n",
            "total_reward: 5268102.62\n",
            "total_cost: 2330466.53\n",
            "total_trades: 68458\n",
            "Sharpe: 0.761\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 269       |\n",
            "|    iterations         | 8900      |\n",
            "|    time_elapsed       | 165       |\n",
            "|    total_timesteps    | 44500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8899      |\n",
            "|    policy_loss        | -66       |\n",
            "|    reward             | 1.6085356 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 2.93      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 9000       |\n",
            "|    time_elapsed       | 167        |\n",
            "|    total_timesteps    | 45000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.7      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8999       |\n",
            "|    policy_loss        | 80.3       |\n",
            "|    reward             | -1.2257614 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 3.26       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 9100       |\n",
            "|    time_elapsed       | 169        |\n",
            "|    total_timesteps    | 45500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.8      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9099       |\n",
            "|    policy_loss        | 12.4       |\n",
            "|    reward             | -1.7819713 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 2.46       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 9200       |\n",
            "|    time_elapsed       | 170        |\n",
            "|    total_timesteps    | 46000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9199       |\n",
            "|    policy_loss        | -23.1      |\n",
            "|    reward             | -0.8992898 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 0.903      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 9300       |\n",
            "|    time_elapsed       | 172        |\n",
            "|    total_timesteps    | 46500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.9      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9299       |\n",
            "|    policy_loss        | -31.9      |\n",
            "|    reward             | -1.2810645 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 12.5       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 269      |\n",
            "|    iterations         | 9400     |\n",
            "|    time_elapsed       | 174      |\n",
            "|    total_timesteps    | 47000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9399     |\n",
            "|    policy_loss        | -184     |\n",
            "|    reward             | 5.873658 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 21.5     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 9500       |\n",
            "|    time_elapsed       | 176        |\n",
            "|    total_timesteps    | 47500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9499       |\n",
            "|    policy_loss        | -283       |\n",
            "|    reward             | -10.777045 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 59.5       |\n",
            "--------------------------------------\n",
            "day: 3397, episode: 14\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5339984.36\n",
            "total_reward: 4339984.36\n",
            "total_cost: 1937624.84\n",
            "total_trades: 70810\n",
            "Sharpe: 0.767\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 269       |\n",
            "|    iterations         | 9600      |\n",
            "|    time_elapsed       | 178       |\n",
            "|    total_timesteps    | 48000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9599      |\n",
            "|    policy_loss        | -2.69     |\n",
            "|    reward             | 1.3272728 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 4.65      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 269       |\n",
            "|    iterations         | 9700      |\n",
            "|    time_elapsed       | 180       |\n",
            "|    total_timesteps    | 48500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9699      |\n",
            "|    policy_loss        | 59.5      |\n",
            "|    reward             | 1.2079386 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 2.12      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 9800       |\n",
            "|    time_elapsed       | 182        |\n",
            "|    total_timesteps    | 49000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43        |\n",
            "|    explained_variance | -0.00196   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9799       |\n",
            "|    policy_loss        | 201        |\n",
            "|    reward             | -3.3131533 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 49.6       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 269         |\n",
            "|    iterations         | 9900        |\n",
            "|    time_elapsed       | 183         |\n",
            "|    total_timesteps    | 49500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -43         |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9899        |\n",
            "|    policy_loss        | -46.7       |\n",
            "|    reward             | -0.46960723 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 5.76        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 269        |\n",
            "|    iterations         | 10000      |\n",
            "|    time_elapsed       | 185        |\n",
            "|    total_timesteps    | 50000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9999       |\n",
            "|    policy_loss        | -13.8      |\n",
            "|    reward             | -0.9619655 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 22.7       |\n",
            "--------------------------------------\n",
            "A2C Validation from 2023-07-06 to 2023-08-30\n",
            "day: 38, episode: 1\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1031806.97\n",
            "total_reward: 31806.97\n",
            "total_cost: 1337749.98\n",
            "total_trades: 840\n",
            "Sharpe: 2.131\n",
            "=================================\n",
            "DDPG Training: \n",
            "Using cuda device\n",
            "day: 3397, episode: 16\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3780091.45\n",
            "total_reward: 2780091.45\n",
            "total_cost: 3655750.41\n",
            "total_trades: 53132\n",
            "Sharpe: 0.620\n",
            "=================================\n",
            "day: 3397, episode: 17\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3871378.43\n",
            "total_reward: 2871378.43\n",
            "total_cost: 1011432.97\n",
            "total_trades: 47596\n",
            "Sharpe: 0.728\n",
            "=================================\n",
            "day: 3397, episode: 18\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3871378.43\n",
            "total_reward: 2871378.43\n",
            "total_cost: 1011432.97\n",
            "total_trades: 47596\n",
            "Sharpe: 0.728\n",
            "=================================\n",
            "day: 3397, episode: 19\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3871378.43\n",
            "total_reward: 2871378.43\n",
            "total_cost: 1011432.97\n",
            "total_trades: 47596\n",
            "Sharpe: 0.728\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 4           |\n",
            "|    fps             | 152         |\n",
            "|    time_elapsed    | 89          |\n",
            "|    total_timesteps | 13592       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 26.3        |\n",
            "|    critic_loss     | 144         |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 10194       |\n",
            "|    reward          | -0.56217265 |\n",
            "------------------------------------\n",
            "day: 3397, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3871378.43\n",
            "total_reward: 2871378.43\n",
            "total_cost: 1011432.97\n",
            "total_trades: 47596\n",
            "Sharpe: 0.728\n",
            "=================================\n",
            "day: 3397, episode: 21\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3871378.43\n",
            "total_reward: 2871378.43\n",
            "total_cost: 1011432.97\n",
            "total_trades: 47596\n",
            "Sharpe: 0.728\n",
            "=================================\n",
            "day: 3397, episode: 22\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3871378.43\n",
            "total_reward: 2871378.43\n",
            "total_cost: 1011432.97\n",
            "total_trades: 47596\n",
            "Sharpe: 0.728\n",
            "=================================\n",
            "day: 3397, episode: 23\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3871378.43\n",
            "total_reward: 2871378.43\n",
            "total_cost: 1011432.97\n",
            "total_trades: 47596\n",
            "Sharpe: 0.728\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 8           |\n",
            "|    fps             | 136         |\n",
            "|    time_elapsed    | 199         |\n",
            "|    total_timesteps | 27184       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 7.1         |\n",
            "|    critic_loss     | 3.13        |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 23786       |\n",
            "|    reward          | -0.56217265 |\n",
            "------------------------------------\n",
            "day: 3397, episode: 24\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3871378.43\n",
            "total_reward: 2871378.43\n",
            "total_cost: 1011432.97\n",
            "total_trades: 47596\n",
            "Sharpe: 0.728\n",
            "=================================\n",
            "day: 3397, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3871378.43\n",
            "total_reward: 2871378.43\n",
            "total_cost: 1011432.97\n",
            "total_trades: 47596\n",
            "Sharpe: 0.728\n",
            "=================================\n",
            "day: 3397, episode: 26\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3871378.43\n",
            "total_reward: 2871378.43\n",
            "total_cost: 1011432.97\n",
            "total_trades: 47596\n",
            "Sharpe: 0.728\n",
            "=================================\n",
            "day: 3397, episode: 27\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3871378.43\n",
            "total_reward: 2871378.43\n",
            "total_cost: 1011432.97\n",
            "total_trades: 47596\n",
            "Sharpe: 0.728\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 12          |\n",
            "|    fps             | 131         |\n",
            "|    time_elapsed    | 309         |\n",
            "|    total_timesteps | 40776       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 1.15        |\n",
            "|    critic_loss     | 1.91        |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 37378       |\n",
            "|    reward          | -0.56217265 |\n",
            "------------------------------------\n",
            "day: 3397, episode: 28\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3871378.43\n",
            "total_reward: 2871378.43\n",
            "total_cost: 1011432.97\n",
            "total_trades: 47596\n",
            "Sharpe: 0.728\n",
            "=================================\n",
            "day: 3397, episode: 29\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3871378.43\n",
            "total_reward: 2871378.43\n",
            "total_cost: 1011432.97\n",
            "total_trades: 47596\n",
            "Sharpe: 0.728\n",
            "=================================\n",
            "day: 3397, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3871378.43\n",
            "total_reward: 2871378.43\n",
            "total_cost: 1011432.97\n",
            "total_trades: 47596\n",
            "Sharpe: 0.728\n",
            "=================================\n",
            "DDPG Validation from 2023-07-06 to 2023-08-30\n",
            "day: 38, episode: 1\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1030320.54\n",
            "total_reward: 30320.54\n",
            "total_cost: 1076689.35\n",
            "total_trades: 541\n",
            "Sharpe: 2.409\n",
            "=================================\n",
            "PPO Training: \n",
            "Using cuda device\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 347       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 5         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 0.4108518 |\n",
            "----------------------------------\n",
            "day: 3397, episode: 32\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3010737.57\n",
            "total_reward: 2010737.57\n",
            "total_cost: 261603179.38\n",
            "total_trades: 94833\n",
            "Sharpe: 0.569\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 326         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016629234 |\n",
            "|    clip_fraction        | 0.201       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | 0.0238      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.86        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0262     |\n",
            "|    reward               | -0.73605734 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 9.88        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 320         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 19          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015301954 |\n",
            "|    clip_fraction        | 0.125       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.00859    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 19.1        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0207     |\n",
            "|    reward               | -3.5406919  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 57.1        |\n",
            "-----------------------------------------\n",
            "day: 3397, episode: 33\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4152433.61\n",
            "total_reward: 3152433.61\n",
            "total_cost: 254003669.05\n",
            "total_trades: 93701\n",
            "Sharpe: 0.733\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 317         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 25          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014868032 |\n",
            "|    clip_fraction        | 0.146       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.00485    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18.2        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0179     |\n",
            "|    reward               | -0.21248658 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 51.6        |\n",
            "-----------------------------------------\n",
            "day: 3397, episode: 34\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3554656.50\n",
            "total_reward: 2554656.50\n",
            "total_cost: 251911868.87\n",
            "total_trades: 93279\n",
            "Sharpe: 0.664\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 316         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 32          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020774823 |\n",
            "|    clip_fraction        | 0.199       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | 0.00362     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.5        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0228     |\n",
            "|    reward               | 0.22350635  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 31.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 315         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 38          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020398635 |\n",
            "|    clip_fraction        | 0.208       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.00139     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 21.5        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0252     |\n",
            "|    reward               | -2.9641922  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 50.1        |\n",
            "-----------------------------------------\n",
            "day: 3397, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5192304.81\n",
            "total_reward: 4192304.81\n",
            "total_cost: 244549045.64\n",
            "total_trades: 92245\n",
            "Sharpe: 0.807\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 314         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 45          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017038949 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.0517     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.34        |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0193     |\n",
            "|    reward               | 1.428626    |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 17          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 313         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 52          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017053584 |\n",
            "|    clip_fraction        | 0.223       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.00459    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 36.6        |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0185     |\n",
            "|    reward               | 0.38975698  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 99.7        |\n",
            "-----------------------------------------\n",
            "day: 3397, episode: 36\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3883132.94\n",
            "total_reward: 2883132.94\n",
            "total_cost: 240466333.89\n",
            "total_trades: 91323\n",
            "Sharpe: 0.668\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 313         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 58          |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015692836 |\n",
            "|    clip_fraction        | 0.172       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | 0.00297     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 30.3        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.016      |\n",
            "|    reward               | 0.13311014  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 47.4        |\n",
            "-----------------------------------------\n",
            "day: 3397, episode: 37\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4644547.14\n",
            "total_reward: 3644547.14\n",
            "total_cost: 243639491.66\n",
            "total_trades: 92088\n",
            "Sharpe: 0.753\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 312         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 65          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024035027 |\n",
            "|    clip_fraction        | 0.252       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | 0.0129      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 22.5        |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0141     |\n",
            "|    reward               | 0.16329128  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 37.8        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 312        |\n",
            "|    iterations           | 11         |\n",
            "|    time_elapsed         | 72         |\n",
            "|    total_timesteps      | 22528      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01754548 |\n",
            "|    clip_fraction        | 0.143      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.5      |\n",
            "|    explained_variance   | 0.00124    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 66.9       |\n",
            "|    n_updates            | 100        |\n",
            "|    policy_gradient_loss | -0.0216    |\n",
            "|    reward               | 1.2277741  |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 120        |\n",
            "----------------------------------------\n",
            "day: 3397, episode: 38\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3978179.72\n",
            "total_reward: 2978179.72\n",
            "total_cost: 246351491.25\n",
            "total_trades: 91986\n",
            "Sharpe: 0.738\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 312         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 78          |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028090388 |\n",
            "|    clip_fraction        | 0.31        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | 0.0289      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.2         |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0262     |\n",
            "|    reward               | -2.527787   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 10          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 312         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 85          |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020485671 |\n",
            "|    clip_fraction        | 0.203       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | 0.0248      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 27.6        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0143     |\n",
            "|    reward               | 0.74548656  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 44.3        |\n",
            "-----------------------------------------\n",
            "day: 3397, episode: 39\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4811571.34\n",
            "total_reward: 3811571.34\n",
            "total_cost: 245142934.54\n",
            "total_trades: 92043\n",
            "Sharpe: 0.792\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 312       |\n",
            "|    iterations           | 14        |\n",
            "|    time_elapsed         | 91        |\n",
            "|    total_timesteps      | 28672     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0246078 |\n",
            "|    clip_fraction        | 0.246     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -41.6     |\n",
            "|    explained_variance   | 0.00166   |\n",
            "|    learning_rate        | 0.00025   |\n",
            "|    loss                 | 16.9      |\n",
            "|    n_updates            | 130       |\n",
            "|    policy_gradient_loss | -0.0176   |\n",
            "|    reward               | 1.6972336 |\n",
            "|    std                  | 1.02      |\n",
            "|    value_loss           | 53.2      |\n",
            "---------------------------------------\n",
            "day: 3397, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4228735.25\n",
            "total_reward: 3228735.25\n",
            "total_cost: 246252235.13\n",
            "total_trades: 91288\n",
            "Sharpe: 0.740\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 311         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 98          |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023662895 |\n",
            "|    clip_fraction        | 0.251       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | -0.00466    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13.2        |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0153     |\n",
            "|    reward               | 1.2274342   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 43.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 311         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 105         |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024835467 |\n",
            "|    clip_fraction        | 0.22        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | 0.0138      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 23.1        |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0184     |\n",
            "|    reward               | 2.9205544   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 62.5        |\n",
            "-----------------------------------------\n",
            "day: 3397, episode: 41\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4942267.50\n",
            "total_reward: 3942267.50\n",
            "total_cost: 240371995.43\n",
            "total_trades: 91010\n",
            "Sharpe: 0.769\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 311         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 111         |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017359937 |\n",
            "|    clip_fraction        | 0.217       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | -0.0172     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.55        |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.018      |\n",
            "|    reward               | -1.0290626  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 19.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 311         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 118         |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023417005 |\n",
            "|    clip_fraction        | 0.188       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | 0.0142      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 57          |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.017      |\n",
            "|    reward               | -9.422982   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 120         |\n",
            "-----------------------------------------\n",
            "day: 3397, episode: 42\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6769984.37\n",
            "total_reward: 5769984.37\n",
            "total_cost: 247418641.59\n",
            "total_trades: 91864\n",
            "Sharpe: 0.878\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 311        |\n",
            "|    iterations           | 19         |\n",
            "|    time_elapsed         | 124        |\n",
            "|    total_timesteps      | 38912      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02560921 |\n",
            "|    clip_fraction        | 0.209      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.9      |\n",
            "|    explained_variance   | -0.0194    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 39.5       |\n",
            "|    n_updates            | 180        |\n",
            "|    policy_gradient_loss | -0.0116    |\n",
            "|    reward               | -2.3044512 |\n",
            "|    std                  | 1.03       |\n",
            "|    value_loss           | 108        |\n",
            "----------------------------------------\n",
            "day: 3397, episode: 43\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4739508.68\n",
            "total_reward: 3739508.68\n",
            "total_cost: 236437044.85\n",
            "total_trades: 90167\n",
            "Sharpe: 0.810\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 311         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 131         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029447367 |\n",
            "|    clip_fraction        | 0.29        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42         |\n",
            "|    explained_variance   | 0.0136      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 39.5        |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.0168     |\n",
            "|    reward               | -0.52992696 |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 75.3        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 311        |\n",
            "|    iterations           | 21         |\n",
            "|    time_elapsed         | 137        |\n",
            "|    total_timesteps      | 43008      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01910058 |\n",
            "|    clip_fraction        | 0.176      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42        |\n",
            "|    explained_variance   | 0.00847    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 36.4       |\n",
            "|    n_updates            | 200        |\n",
            "|    policy_gradient_loss | -0.017     |\n",
            "|    reward               | -0.5889634 |\n",
            "|    std                  | 1.03       |\n",
            "|    value_loss           | 70.5       |\n",
            "----------------------------------------\n",
            "day: 3397, episode: 44\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4937827.56\n",
            "total_reward: 3937827.56\n",
            "total_cost: 222028869.83\n",
            "total_trades: 88870\n",
            "Sharpe: 0.784\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 311        |\n",
            "|    iterations           | 22         |\n",
            "|    time_elapsed         | 144        |\n",
            "|    total_timesteps      | 45056      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02411028 |\n",
            "|    clip_fraction        | 0.27       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42        |\n",
            "|    explained_variance   | -0.0668    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 4.86       |\n",
            "|    n_updates            | 210        |\n",
            "|    policy_gradient_loss | -0.0229    |\n",
            "|    reward               | 0.35446367 |\n",
            "|    std                  | 1.03       |\n",
            "|    value_loss           | 11.7       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 308         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 152         |\n",
            "|    total_timesteps      | 47104       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023785539 |\n",
            "|    clip_fraction        | 0.226       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | 0.0247      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 39.7        |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.0159     |\n",
            "|    reward               | 0.4568647   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 83.2        |\n",
            "-----------------------------------------\n",
            "day: 3397, episode: 45\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4483399.39\n",
            "total_reward: 3483399.39\n",
            "total_cost: 225182999.36\n",
            "total_trades: 89425\n",
            "Sharpe: 0.738\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 309         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 159         |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022480812 |\n",
            "|    clip_fraction        | 0.249       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | 0.00909     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 27.4        |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0125     |\n",
            "|    reward               | 0.82307845  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 49.2        |\n",
            "-----------------------------------------\n",
            "day: 3397, episode: 46\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6162636.31\n",
            "total_reward: 5162636.31\n",
            "total_cost: 234530445.61\n",
            "total_trades: 89630\n",
            "Sharpe: 0.899\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 309         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 165         |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027815029 |\n",
            "|    clip_fraction        | 0.227       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | 0.0236      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.1         |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.0163     |\n",
            "|    reward               | 1.8561431   |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 33.2        |\n",
            "-----------------------------------------\n",
            "PPO Validation from 2023-07-06 to 2023-08-30\n",
            "day: 38, episode: 1\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1006528.87\n",
            "total_reward: 6528.87\n",
            "total_cost: 5385444.18\n",
            "total_trades: 947\n",
            "Sharpe: 0.543\n",
            "=================================\n",
            "Ensemble Model Training: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "UVrc6_3lpBgC",
        "outputId": "343e57cb-e58d-42f6-cf77-d5e908c33aa9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   iteration  Start Date    End Date model_order  a2c_sharpe  ddpg_sharpe  \\\n",
              "0        126  2023-01-03  2023-04-04        ddpg    0.151331     0.307817   \n",
              "1        189  2023-04-04  2023-07-06         a2c    0.184861     0.147493   \n",
              "2        252  2023-07-06  2023-08-30        ddpg    0.268472     0.303560   \n",
              "\n",
              "   ppo_sharpe  \n",
              "0   -0.122026  \n",
              "1   -0.026791  \n",
              "2    0.068405  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b9508a8c-99ad-4c97-9d8f-e2dcf3fec05f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iteration</th>\n",
              "      <th>Start Date</th>\n",
              "      <th>End Date</th>\n",
              "      <th>model_order</th>\n",
              "      <th>a2c_sharpe</th>\n",
              "      <th>ddpg_sharpe</th>\n",
              "      <th>ppo_sharpe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>126</td>\n",
              "      <td>2023-01-03</td>\n",
              "      <td>2023-04-04</td>\n",
              "      <td>ddpg</td>\n",
              "      <td>0.151331</td>\n",
              "      <td>0.307817</td>\n",
              "      <td>-0.122026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>189</td>\n",
              "      <td>2023-04-04</td>\n",
              "      <td>2023-07-06</td>\n",
              "      <td>a2c</td>\n",
              "      <td>0.184861</td>\n",
              "      <td>0.147493</td>\n",
              "      <td>-0.026791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>252</td>\n",
              "      <td>2023-07-06</td>\n",
              "      <td>2023-08-30</td>\n",
              "      <td>ddpg</td>\n",
              "      <td>0.268472</td>\n",
              "      <td>0.303560</td>\n",
              "      <td>0.068405</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b9508a8c-99ad-4c97-9d8f-e2dcf3fec05f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b9508a8c-99ad-4c97-9d8f-e2dcf3fec05f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b9508a8c-99ad-4c97-9d8f-e2dcf3fec05f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fd7a6b79-bb39-4681-bcf3-239e37f5367b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fd7a6b79-bb39-4681-bcf3-239e37f5367b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fd7a6b79-bb39-4681-bcf3-239e37f5367b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_ea01a957-cbee-4a62-9b05-ae71152f2d61\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summary')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ea01a957-cbee-4a62-9b05-ae71152f2d61 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('summary');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}